{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d23924",
   "metadata": {},
   "source": [
    "# Music Recommendation System\n",
    "## Part 2: Modeling & Recommendation Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8107be",
   "metadata": {},
   "source": [
    "This notebook implements **various embedding techniques and recommendation strategies** for our content-based music recommendation system. We compare TF-IDF, Word2Vec, FastText, and Sentence-BERT embeddings for lyric-based song similarity.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "### Core Implementation (Completed)\n",
    "1. [Import Libraries](#1-import-libraries)\n",
    "2. [Load Dataset](#2-load-dataset)\n",
    "3. [Baseline: TF-IDF](#3-baseline-tf-idf)\n",
    "4. [Building FAISS Index](#4-building-faiss-index-baseline)\n",
    "5. [Recommendation Function](#5-recommendation-function)\n",
    "6. [Model Persistence](#6-model-persistence)\n",
    "\n",
    "### Advanced Embeddings (Completed)\n",
    "7. [Word2Vec & FastText](#7-improvement-1-word2vec--fasttext)\n",
    "   - [7.1 Tokenize Lyrics for Word Embeddings](#71-tokenize-lyrics-for-word-embeddings)\n",
    "   - [7.2 Train Custom Word2Vec Model](#72-train-custom-word2vec-model)\n",
    "   - [7.3 Document Embedding Function](#73-document-embedding-function)\n",
    "   - [7.4 Build Document Embeddings for All Songs](#74-build-document-embeddings-for-all-songs)\n",
    "   - [7.5 Train Custom FastText Model](#75-train-custom-fasttext-model)\n",
    "   - [7.6 Explore Semantic Relationships](#76-explore-semantic-relationships)\n",
    "   - [7.7 Build FastText Document Embeddings](#77-build-fasttext-document-embeddings)\n",
    "   - [7.8 Build FAISS Indexes for Semantic Search](#78-build-faiss-indexes-for-semantic-search)\n",
    "   - [7.9 Semantic Recommendation Functions](#79-semantic-recommendation-functions)\n",
    "   - [7.10 Test the Recommendation System](#710-test-the-recommendation-system)\n",
    "   - [7.11 Save Semantic Models](#711-save-semantic-models)\n",
    "\n",
    "8. [Sentence-BERT](#8-improvement-2-sentence-bert)\n",
    "   - [8.1 Load Sentence-Transformers](#81-load-sentence-transformers)\n",
    "   - [8.2 Generate SBERT Embeddings for All Songs](#82-generate-sbert-embeddings-for-all-songs)\n",
    "   - [8.3 Build FAISS Index for SBERT Embeddings](#83-build-faiss-index-for-sbert-embeddings)\n",
    "   - [8.4 SBERT Recommendation Function](#84-sbert-recommendation-function)\n",
    "   - [8.5 Test the Recommendation System](#85-test-the-recommendation-system)\n",
    "   - [8.6 Save SBERT Models and Embeddings](#86-save-sbert-models-and-embeddings)\n",
    "\n",
    "### Future Enhancements (Optional)\n",
    "9. [Audio Features (Spotify API)](#9-improvement-3-audio-features-spotify-api) - *TODO*\n",
    "10. [Embedding Comparison & Evaluation](#10-embedding-comparison--evaluation) - *TODO*\n",
    "11. [Hybrid Recommendation System](#11-hybrid-recommendation-system) - *TODO*\n",
    "12. [Advanced FAISS Indexes](#12-advanced-faiss-indexes) - *TODO*\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**Implemented Models:**\n",
    "- **TF-IDF**: Baseline bag-of-words approach (5000 dimensions)\n",
    "- **Word2Vec**: Custom-trained on lyrics dataset (100 dimensions)\n",
    "- **FastText**: Custom-trained with subword support (100 dimensions)\n",
    "- **SBERT**: Sentence-BERT contextual embeddings (384 dimensions)\n",
    "\n",
    "**All models are saved and ready for deployment!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb61039",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3817588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import faiss\n",
    "import pickle\n",
    "from scipy.sparse import save_npz\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import HfApi, login\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7689de0f",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "243b8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/spotify_millsongdata_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b666ddb",
   "metadata": {},
   "source": [
    "## 3. Baseline: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0cf621",
   "metadata": {},
   "source": [
    "> **Baseline Approach**: We start with TF-IDF (Term Frequency-Inverse Document Frequency) as our baseline embedding method. This is a classic approach for text similarity that works well but has limitations in capturing semantic meaning.\n",
    "\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency)** is a numerical statistic that reflects how important a word is to a document in a collection.\n",
    "\n",
    "- **Term Frequency (TF)**: How often a word appears in a document\n",
    "\n",
    "$$\n",
    "tf(t, d) = \\frac{f_{t,d}}{\\sum_{k} f_{k,d}}\n",
    "$$\n",
    "\n",
    "where $f_{t,d}$ denotes the number of occurrences of term $t$ in document $d$.\n",
    "\n",
    "- **Inverse Document Frequency (IDF)**: How rare a word is across all documents\n",
    "\n",
    "$$\n",
    "idf(t, D) = \\log \\left( \\frac{N}{1 + n_t} \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $N$ is the total number of documents in the corpus,\n",
    "- $n_t$ is the number of documents containing term $t$.\n",
    "\n",
    "\n",
    "**TF-IDF Weight**\n",
    "\n",
    "The final TF-IDF weight of term $t$ in document $d$ is computed as:\n",
    "\n",
    "$$\n",
    "w_{t,d} = tf(t, d) \\times idf(t, D)\n",
    "$$\n",
    "\n",
    "Words that appear frequently in one song but rarely across all songs get higher weights, making them better for distinguishing between songs.\n",
    "\n",
    "We limit to `max_features=5000` to keep the most important terms and reduce dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ffa898b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape: (57650, 5000)\n",
      "Vocabulary Size: 5000\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, dtype=np.float32)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
    "\n",
    "print(f\"TF-IDF Matrix Shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Vocabulary Size: {len(tfidf_vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a54a54c",
   "metadata": {},
   "source": [
    "## 4. Building FAISS Index (Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b0a9c9",
   "metadata": {},
   "source": [
    "**FAISS (Facebook AI Similarity Search)** is a library for efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "### **Why FAISS?**\n",
    "- Traditional cosine similarity has O(n) complexity for each query\n",
    "- FAISS uses optimized algorithms for faster nearest neighbor search\n",
    "- Essential for scaling to large datasets (millions of songs)\n",
    "\n",
    "---\n",
    "\n",
    "### **Index Type: `IndexFlatIP`**\n",
    "- **IP** = Inner Product (equivalent to cosine similarity for L2-normalized vectors)\n",
    "- **Flat** = Exact search (no approximation)\n",
    "- We normalize vectors with L2 norm so inner product equals cosine similarity\n",
    "\n",
    "#### **$L_2$ Normalization**\n",
    "\n",
    "$L_2$ normalization transforms a document vector  $\\mathbf{x} = [x_1, x_2, \\dots, x_n]$  into a unit vector $\\hat{\\mathbf{x}}$ with Euclidean norm equal to 1:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}} = \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2}\n",
    "= \\frac{\\mathbf{x}}{\\sqrt{\\sum_{i=1}^{n} x_i^2}}\n",
    "$$\n",
    "\n",
    "##### **Geometric Interpretation**\n",
    "\n",
    "After $L_2$ normalization, all document vectors lie on the surface of a **unit hypersphere**. This normalization removes the influence of document length, ensuring that similarity comparisons depend only on the **direction** of the vectors (semantic content), rather than their magnitude.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Similarity Search with FAISS and Inner Product**\n",
    "\n",
    "In FAISS, `IndexFlatIP` performs similarity search based on the **Inner Product (IP)**.  \n",
    "Given a query vector $\\mathbf{q}$ and a set of document vectors $\\mathbf{X}$ stored in the index, the similarity score $s_i$ is computed as:\n",
    "\n",
    "$$\n",
    "s_i = \\mathbf{q} \\cdot \\mathbf{x}_i\n",
    "= \\sum_{j=1}^{dim} q_j \\cdot x_{i,j}\n",
    "$$\n",
    "\n",
    "##### **Relationship Between Inner Product and Cosine Similarity**\n",
    "\n",
    "Cosine Similarity between two vectors $\\mathbf{q}$ and $\\mathbf{x}$ is defined as:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_sim}(\\mathbf{q}, \\mathbf{x})\n",
    "= \\frac{\\mathbf{q} \\cdot \\mathbf{x}}{\\|\\mathbf{q}\\|_2 \\, \\|\\mathbf{x}\\|_2}\n",
    "$$\n",
    "\n",
    "Since both the query vector and document vectors are $L_2$-normalized  ($\\|\\mathbf{q}\\|_2 = 1$ and $\\|\\mathbf{x}\\|_2 = 1$), the equation simplifies to:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_sim}(\\mathbf{q}, \\mathbf{x})\n",
    "= \\mathbf{q} \\cdot \\mathbf{x}\n",
    "= \\text{Inner Product}\n",
    "$$\n",
    "\n",
    "Therefore, when using `IndexFlatIP` on $L_2$-normalized TF-IDF vectors, FAISS effectively computes **Cosine Similarity**, representing the degree of semantic similarity between the query text and the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc72d6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Dimension: 5000\n",
      "Total vectors in FAISS index: 57650\n",
      "FAISS index built successfully!\n"
     ]
    }
   ],
   "source": [
    "X = tfidf_matrix.toarray()\n",
    "X = normalize(X, norm=\"l2\")\n",
    "\n",
    "dim = X.shape[1]\n",
    "print(f\"Vector Dimension: {dim}\")\n",
    "\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(X)\n",
    "\n",
    "print(f\"Total vectors in FAISS index: {index.ntotal}\")\n",
    "print(\"FAISS index built successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd46207",
   "metadata": {},
   "source": [
    "## 5. Recommendation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b842852c",
   "metadata": {},
   "source": [
    "The core recommendation logic:\n",
    "\n",
    "1. **Find the song**: Look up the input song name in our dataset\n",
    "2. **Get its vector**: Retrieve the TF-IDF vector for that song\n",
    "3. **Search similar songs**: Use FAISS to find the k most similar songs\n",
    "4. **Return results**: Return song details (excluding the input song itself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a18d0344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_songs_tfidf(song_name, df=df, top_k=5):\n",
    "\n",
    "    idx = df[df['song'].str.lower() == song_name.lower()].index\n",
    "    if len(idx) == 0:\n",
    "        return \"Song not found in the dataset.\"\n",
    "    idx = idx[0]\n",
    "\n",
    "    query = X[idx].reshape(1, -1)\n",
    "\n",
    "    scores, indices = index.search(query, top_k + 1)\n",
    "\n",
    "    results = df[['song', 'artist']].iloc[indices[0][1:]].copy()\n",
    "    results['similarity'] = scores[0][1:]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81acee30",
   "metadata": {},
   "source": [
    "### Test the Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10d9651",
   "metadata": {},
   "source": [
    "Let's test our recommendation system by getting similar songs for the first song in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e0a9338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting recommendations for: 'All Those Years Ago' by George Harrison\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5860</th>\n",
       "      <td>Stone</td>\n",
       "      <td>Faces</td>\n",
       "      <td>0.603980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13920</th>\n",
       "      <td>Ultraviolence</td>\n",
       "      <td>New Order</td>\n",
       "      <td>0.472739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4824</th>\n",
       "      <td>Two Years Ago</td>\n",
       "      <td>Ellie Goulding</td>\n",
       "      <td>0.452838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42284</th>\n",
       "      <td>Remember Me</td>\n",
       "      <td>Lucky Dube</td>\n",
       "      <td>0.438898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51099</th>\n",
       "      <td>It Wasn't Very Long Ago</td>\n",
       "      <td>Roy Orbison</td>\n",
       "      <td>0.397654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          song          artist  similarity\n",
       "5860                     Stone           Faces    0.603980\n",
       "13920            Ultraviolence       New Order    0.472739\n",
       "4824             Two Years Ago  Ellie Goulding    0.452838\n",
       "42284              Remember Me      Lucky Dube    0.438898\n",
       "51099  It Wasn't Very Long Ago     Roy Orbison    0.397654"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = random.randint(0, len(df))\n",
    "test_song = df['song'].iloc[idx]\n",
    "test_artist = df['artist'].iloc[idx]\n",
    "print(f\"Getting recommendations for: '{test_song}' by {test_artist}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "display(recommend_songs_tfidf(test_song))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0840d7a",
   "metadata": {},
   "source": [
    "## 6. Model Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dd1479",
   "metadata": {},
   "source": [
    "Save the trained models and processed data for later use:\n",
    "\n",
    "- **df_cleaned.pkl**: Preprocessed DataFrame with cleaned lyrics\n",
    "- **faiss_index.pkl**: FAISS index for fast similarity search\n",
    "- **tfidf_matrix.pkl**: TF-IDF vectors for all songs\n",
    "\n",
    "These files can be loaded later to make recommendations without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60cd5729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: df_cleaned.parquet\n",
      "Saved: embeddings_tfidf.npz\n",
      "Saved: faiss_tfidf.index\n",
      "\n",
      "All models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "df.to_parquet(\"../data/processed/df_cleaned.parquet\", index=False)\n",
    "print(\"Saved: df_cleaned.parquet\")\n",
    "\n",
    "save_npz(\"../data/embeddings/embeddings_tfidf.npz\", tfidf_matrix)\n",
    "print(\"Saved: embeddings_tfidf.npz\")\n",
    "\n",
    "faiss.write_index(index, \"../models/faiss_indexes/faiss_tfidf.index\")\n",
    "print(\"Saved: faiss_tfidf.index\")\n",
    "\n",
    "print(\"\\nAll models saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f45ff0b",
   "metadata": {},
   "source": [
    "---\n",
    "# IMPROVEMENTS\n",
    "\n",
    "The following sections implement advanced techniques to improve recommendation quality and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f80cc3b",
   "metadata": {},
   "source": [
    "## 7. Improvement 1: Word2Vec / FastText\n",
    "\n",
    "> **Goal**: Capture semantic similarity between words. Unlike TF-IDF which treats words as independent tokens, word embeddings represent words in a continuous vector space where semantically similar words are close together.\n",
    "\n",
    "### Why Word Embeddings?\n",
    "\n",
    "**Limitation of TF-IDF vs Solution with Word2Vec/FastText:**\n",
    "- \"happy\" and \"joyful\" are completely different features in TF-IDF, but similar vectors in Word2Vec\n",
    "- TF-IDF has no semantic understanding, Word2Vec captures word meaning\n",
    "- OOV words are ignored in TF-IDF, FastText handles via subwords\n",
    "\n",
    "### Approaches\n",
    "\n",
    "1. **Pre-trained Word2Vec** (Google News 300d)\n",
    "   - 3M words, 300 dimensions\n",
    "   - General semantic knowledge\n",
    "   \n",
    "2. **Custom FastText** (trained on lyrics)\n",
    "   - Domain-specific vocabulary\n",
    "   - Handles misspellings and slang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df00ecac",
   "metadata": {},
   "source": [
    "### 7.1 Tokenize Lyrics for Word Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e851e1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\r\\nA...</td>\n",
       "      <td>[look, face, wonderful, face, means, something...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>Take it easy with me, please  \\r\\nTouch me gen...</td>\n",
       "      <td>[take, easy, please, touch, gently, like, summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As Good As New</td>\n",
       "      <td>I'll never know why I had to go  \\r\\nWhy I had...</td>\n",
       "      <td>[ill, never, know, go, put, lousy, rotten, sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bang</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>[making, somebody, happy, question, give, take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>[making, somebody, happy, question, give, take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57645</th>\n",
       "      <td>Good Old Days</td>\n",
       "      <td>Irie days come on play  \\r\\nLet the angels fly...</td>\n",
       "      <td>[irie, days, come, play, let, angels, fly, let...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57646</th>\n",
       "      <td>Hand To Mouth</td>\n",
       "      <td>Power to the workers  \\r\\nMore power  \\r\\nPowe...</td>\n",
       "      <td>[power, workers, power, power, workers, need, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57647</th>\n",
       "      <td>Come With Me</td>\n",
       "      <td>all you need  \\r\\nis something i'll believe  \\...</td>\n",
       "      <td>[need, something, ill, believe, flashlights, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57648</th>\n",
       "      <td>Desire</td>\n",
       "      <td>northern star  \\r\\nam i frightened  \\r\\nwhere ...</td>\n",
       "      <td>[northern, star, frightened, go, rest, cant, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57649</th>\n",
       "      <td>Heartsong</td>\n",
       "      <td>come in  \\r\\nmake yourself at home  \\r\\ni'm a ...</td>\n",
       "      <td>[come, make, home, im, bit, late, hate, make, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57650 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        song  \\\n",
       "0      Ahe's My Kind Of Girl   \n",
       "1           Andante, Andante   \n",
       "2             As Good As New   \n",
       "3                       Bang   \n",
       "4           Bang-A-Boomerang   \n",
       "...                      ...   \n",
       "57645          Good Old Days   \n",
       "57646          Hand To Mouth   \n",
       "57647           Come With Me   \n",
       "57648                 Desire   \n",
       "57649              Heartsong   \n",
       "\n",
       "                                                    text  \\\n",
       "0      Look at her face, it's a wonderful face  \\r\\nA...   \n",
       "1      Take it easy with me, please  \\r\\nTouch me gen...   \n",
       "2      I'll never know why I had to go  \\r\\nWhy I had...   \n",
       "3      Making somebody happy is a question of give an...   \n",
       "4      Making somebody happy is a question of give an...   \n",
       "...                                                  ...   \n",
       "57645  Irie days come on play  \\r\\nLet the angels fly...   \n",
       "57646  Power to the workers  \\r\\nMore power  \\r\\nPowe...   \n",
       "57647  all you need  \\r\\nis something i'll believe  \\...   \n",
       "57648  northern star  \\r\\nam i frightened  \\r\\nwhere ...   \n",
       "57649  come in  \\r\\nmake yourself at home  \\r\\ni'm a ...   \n",
       "\n",
       "                                                  tokens  \n",
       "0      [look, face, wonderful, face, means, something...  \n",
       "1      [take, easy, please, touch, gently, like, summ...  \n",
       "2      [ill, never, know, go, put, lousy, rotten, sho...  \n",
       "3      [making, somebody, happy, question, give, take...  \n",
       "4      [making, somebody, happy, question, give, take...  \n",
       "...                                                  ...  \n",
       "57645  [irie, days, come, play, let, angels, fly, let...  \n",
       "57646  [power, workers, power, power, workers, need, ...  \n",
       "57647  [need, something, ill, believe, flashlights, h...  \n",
       "57648  [northern, star, frightened, go, rest, cant, s...  \n",
       "57649  [come, make, home, im, bit, late, hate, make, ...  \n",
       "\n",
       "[57650 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'] = df['cleaned_text'].apply(lambda x: x.split())\n",
    "\n",
    "df[['song', 'text', 'tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc09e743",
   "metadata": {},
   "source": [
    "### 7.2 Train Custom Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77b949f",
   "metadata": {},
   "source": [
    "We train a Word2Vec model directly on our lyrics dataset to capture domain-specific semantic relationships.\n",
    "\n",
    "**Why Custom Training?**\n",
    "- Domain-specific vocabulary (music lyrics, slang, artist names)\n",
    "- Smaller model size (100 dimensions vs 300)\n",
    "- Better understanding of music-specific terms\n",
    "- No external dependencies or large downloads\n",
    "\n",
    "**Training Parameters:**\n",
    "- `vector_size=100`: Dimension of word vectors (balanced between quality and efficiency)\n",
    "- `window=5`: Context window size (5 words before and after)\n",
    "- `min_count=2`: Ignore words that appear less than 2 times\n",
    "- `workers=4`: Parallel training with 4 threads\n",
    "- `sg=1`: Use Skip-gram algorithm (better for small datasets)\n",
    "- `epochs=10`: Number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e01f3287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model: 'Word2Vec' object has no attribute 'key_to_index'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # w2v_model = api.load('word2vec-google-news-300')\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=df['tokens'].to_list(),\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=2,\n",
    "        workers=4,\n",
    "        sg=1,\n",
    "        epochs=10\n",
    "    )\n",
    "    print(f\"Vocabulary size: {len(w2v_model.key_to_index):,}\")\n",
    "    print(f\"Vector dimension: {w2v_model.vector_size}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45b0c17",
   "metadata": {},
   "source": [
    "### 7.3 Document Embedding Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12337ac3",
   "metadata": {},
   "source": [
    "To get a single vector representation for an entire song (document), we compute the **weighted average** of word vectors:\n",
    "\n",
    "$$\\vec{d} = \\frac{1}{|W|} \\sum_{w \\in W} \\vec{w}$$\n",
    "\n",
    "where $W$ is the set of words in the document and $\\vec{w}$ is the word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7967f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_embedding(tokens, model):\n",
    "    \"\"\"\n",
    "    Compute document embedding by averaging word vectors.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of words in the document\n",
    "        model: Word2Vec/FastText model\n",
    "        vector_size: Dimension of word vectors\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (vector_size,) - document embedding\n",
    "    \"\"\"\n",
    "\n",
    "    vector_size = model.vector_size\n",
    "    word_vectors = []\n",
    "    for word in tokens:\n",
    "        if word in model:\n",
    "            word_vectors.append(model[word])\n",
    "     \n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "    return np.mean(word_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a567b1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document embedding shape: (100,)\n",
      "First 10 values: [-0.32375708  0.17338105  0.09558945  0.15887557  0.05994973 -0.07022716\n",
      "  0.2102842   0.32940835 -0.27039382 -0.16260444]\n"
     ]
    }
   ],
   "source": [
    "sample_tokens = df['tokens'].iloc[0]\n",
    "sample_embedding = get_document_embedding(sample_tokens, w2v_model.wv)\n",
    "print(f\"Sample document embedding shape: {sample_embedding.shape}\")\n",
    "print(f\"First 10 values: {sample_embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948da8b8",
   "metadata": {},
   "source": [
    "### 7.4 Build Document Embeddings for All Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5ebcad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing songs: 100%|██████████| 57650/57650 [00:08<00:00, 6989.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document embeddings shape: (57650, 100)\n",
      "Memory usage: 21.99 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_embedding_w2v = []\n",
    "for tokens in tqdm(df['tokens'], desc=\"Processing songs\"):\n",
    "    embedding = get_document_embedding(tokens, w2v_model.wv)\n",
    "    doc_embedding_w2v.append(embedding)\n",
    "\n",
    "X_w2v = np.array(doc_embedding_w2v, dtype=np.float32)\n",
    "\n",
    "print(f\"Document embeddings shape: {X_w2v.shape}\")\n",
    "print(f\"Memory usage: {X_w2v.nbytes / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b650cbf3",
   "metadata": {},
   "source": [
    "### 7.5 Train Custom FastText Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f80b72",
   "metadata": {},
   "source": [
    "FastText extends Word2Vec by representing words as bags of character n-grams. This allows it to:\n",
    "- Handle **out-of-vocabulary (OOV)** words by composing subword vectors\n",
    "- Better handle **misspellings** and **slang** common in lyrics\n",
    "- Capture **morphological** relationships (e.g., \"love\", \"loving\", \"loved\")\n",
    "\n",
    "**Training Parameters:**\n",
    "- `vector_size=100`: Dimension of word vectors (smaller for faster training)\n",
    "- `window=5`: Context window size\n",
    "- `min_count=2`: Ignore words that appear less than 2 times\n",
    "- `epochs=10`: Number of training iterations\n",
    "- `sg=1`: Use Skip-gram (better for small datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f75235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText model trained successfully!\n",
      "Vocabulary size: 52,953\n",
      "Vector dimension: 100\n"
     ]
    }
   ],
   "source": [
    "fasttext_model = FastText(\n",
    "    sentences=df['tokens'].to_list(),\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "print(f\"FastText model trained successfully!\")\n",
    "print(f\"Vocabulary size: {len(fasttext_model.wv.key_to_index):,}\")\n",
    "print(f\"Vector dimension: {fasttext_model.wv.vector_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a1ecf5",
   "metadata": {},
   "source": [
    "### 7.6 Explore Semantic Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d071691a",
   "metadata": {},
   "source": [
    "Let's verify that our FastText model captured meaningful semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "845c6890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to common lyrics terms:\n",
      "\n",
      "'love' → itlove (0.773), ohlove (0.767), lovewe (0.760), mylove (0.756), alove (0.755)\n",
      "'heart' → heartfelt (0.786), heartmy (0.785), heartheart (0.767), hearts (0.736), hearttheres (0.724)\n",
      "'dance' → dancego (0.824), dancefloor (0.759), decadance (0.758), danceevery (0.743), danceflo (0.736)\n",
      "'night' → nightgo (0.821), nightynight (0.821), nightoh (0.803), nightgive (0.791), nightnight (0.786)\n",
      "'baby' → bbaby (0.805), ohbaby (0.797), babyi (0.792), bbbbaby (0.789), ofbaby (0.784)\n"
     ]
    }
   ],
   "source": [
    "test_words = ['love', 'heart', 'dance', 'night', 'baby']\n",
    "\n",
    "print(\"Words most similar to common lyrics terms:\\n\")\n",
    "for word in test_words:\n",
    "    try:\n",
    "        similar = fasttext_model.wv.most_similar(word, topn=5)\n",
    "        similar_words = [f\"{w} ({s:.3f})\" for w, s in similar]\n",
    "        print(f\"'{word}' → {', '.join(similar_words)}\")\n",
    "    except KeyError:\n",
    "        print(f\"'{word}' not in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e1376",
   "metadata": {},
   "source": [
    "### 7.7 Build FastText Document Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e990fd17",
   "metadata": {},
   "source": [
    "Now let's create document embeddings using our trained FastText model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7493765a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing FastText document embeddings for all songs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing songs: 100%|██████████| 57650/57650 [00:10<00:00, 5704.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FastText document embeddings shape: (57650, 100)\n",
      "Memory usage: 21.99 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing FastText document embeddings for all songs...\")\n",
    "\n",
    "doc_embeddings_ft = []\n",
    "for tokens in tqdm(df['tokens'], desc=\"Processing songs\"):\n",
    "    embedding = get_document_embedding(tokens, fasttext_model.wv)\n",
    "    doc_embeddings_ft.append(embedding)\n",
    "\n",
    "X_ft = np.array(doc_embeddings_ft, dtype=np.float32)\n",
    "\n",
    "print(f\"\\nFastText document embeddings shape: {X_ft.shape}\")\n",
    "print(f\"Memory usage: {X_ft.nbytes / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c6a3cf",
   "metadata": {},
   "source": [
    "### 7.8 Build FAISS Indexes for Semantic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd4bbca",
   "metadata": {},
   "source": [
    "Create FAISS indexes for both Word2Vec and FastText embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8afb267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FAISS index for Word2Vec embeddings...\n",
      "Word2Vec FAISS index: 57650 vectors, 100 dimensions\n",
      "\n",
      "Building FAISS index for FastText embeddings...\n",
      "FastText FAISS index: 57650 vectors, 100 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Normalize embeddings for cosine similarity\n",
    "X_w2v_norm = normalize(X_w2v, norm='l2')\n",
    "X_ft_norm = normalize(X_ft, norm='l2')\n",
    "\n",
    "# Build FAISS index for Word2Vec embeddings\n",
    "print(\"Building FAISS index for Word2Vec embeddings...\")\n",
    "index_w2v = faiss.IndexFlatIP(X_w2v_norm.shape[1])\n",
    "index_w2v.add(X_w2v_norm)\n",
    "print(f\"Word2Vec FAISS index: {index_w2v.ntotal} vectors, {X_w2v_norm.shape[1]} dimensions\")\n",
    "\n",
    "# Build FAISS index for FastText embeddings\n",
    "print(\"\\nBuilding FAISS index for FastText embeddings...\")\n",
    "index_ft = faiss.IndexFlatIP(X_ft_norm.shape[1])\n",
    "index_ft.add(X_ft_norm)\n",
    "print(f\"FastText FAISS index: {index_ft.ntotal} vectors, {X_ft_norm.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238d7ab3",
   "metadata": {},
   "source": [
    "### 7.9 Semantic Recommendation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7c6c76",
   "metadata": {},
   "source": [
    "Create recommendation functions using Word2Vec and FastText embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "106ded4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_songs_w2v(song_name, df=df, top_k=5):\n",
    "    \"\"\"\n",
    "    Recommend songs using Word2Vec embeddings.\n",
    "    \"\"\"\n",
    "    idx = df[df['song'].str.lower() == song_name.lower()].index\n",
    "    if len(idx) == 0:\n",
    "        return \"Song not found in the dataset.\"\n",
    "    idx = idx[0]\n",
    "    \n",
    "    query = X_w2v_norm[idx].reshape(1, -1)\n",
    "    scores, indices = index_w2v.search(query, top_k + 1)\n",
    "    \n",
    "    result = df[['song', 'artist']].iloc[indices[0][1:]].copy()\n",
    "    result['similarity'] = scores[0][1:]\n",
    "    return result\n",
    "\n",
    "\n",
    "def recommend_songs_fasttext(song_name, df=df, top_k=5):\n",
    "    \"\"\"\n",
    "    Recommend songs using FastText embeddings.\n",
    "    \"\"\"\n",
    "    idx = df[df['song'].str.lower() == song_name.lower()].index\n",
    "    if len(idx) == 0:\n",
    "        return \"Song not found in the dataset.\"\n",
    "    idx = idx[0]\n",
    "    \n",
    "    query = X_ft_norm[idx].reshape(1, -1)\n",
    "    scores, indices = index_ft.search(query, top_k + 1)\n",
    "    \n",
    "    result = df[['song', 'artist']].iloc[indices[0][1:]].copy()\n",
    "    result['similarity'] = scores[0][1:]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4485e6f1",
   "metadata": {},
   "source": [
    "### 7.10 Test the Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e757295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for: 'Chaos At The Airport' by Chaos At The Airport\n",
      "======================================================================\n",
      "\n",
      "Word2Vec Recommendations:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14559</th>\n",
       "      <td>The Last Time</td>\n",
       "      <td>O.A.R.</td>\n",
       "      <td>0.964723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33405</th>\n",
       "      <td>Alien Afternoon</td>\n",
       "      <td>Genesis</td>\n",
       "      <td>0.964573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37362</th>\n",
       "      <td>From Silver Lake</td>\n",
       "      <td>Jackson Browne</td>\n",
       "      <td>0.963711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20095</th>\n",
       "      <td>It's Dangerous Business Walking Out Your Front...</td>\n",
       "      <td>Underoath</td>\n",
       "      <td>0.963548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48107</th>\n",
       "      <td>More Than This</td>\n",
       "      <td>Peter Gabriel</td>\n",
       "      <td>0.962642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    song          artist  \\\n",
       "14559                                      The Last Time          O.A.R.   \n",
       "33405                                    Alien Afternoon         Genesis   \n",
       "37362                                   From Silver Lake  Jackson Browne   \n",
       "20095  It's Dangerous Business Walking Out Your Front...       Underoath   \n",
       "48107                                     More Than This   Peter Gabriel   \n",
       "\n",
       "       similarity  \n",
       "14559    0.964723  \n",
       "33405    0.964573  \n",
       "37362    0.963711  \n",
       "20095    0.963548  \n",
       "48107    0.962642  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FastText Recommendations:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33405</th>\n",
       "      <td>Alien Afternoon</td>\n",
       "      <td>Genesis</td>\n",
       "      <td>0.967511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14559</th>\n",
       "      <td>The Last Time</td>\n",
       "      <td>O.A.R.</td>\n",
       "      <td>0.963286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12472</th>\n",
       "      <td>The Great Escape</td>\n",
       "      <td>Marillion</td>\n",
       "      <td>0.963130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34448</th>\n",
       "      <td>Black Throated Wind</td>\n",
       "      <td>Grateful Dead</td>\n",
       "      <td>0.961648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45512</th>\n",
       "      <td>Waiting For The Sirens' Call</td>\n",
       "      <td>New Order</td>\n",
       "      <td>0.961460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               song         artist  similarity\n",
       "33405               Alien Afternoon        Genesis    0.967511\n",
       "14559                 The Last Time         O.A.R.    0.963286\n",
       "12472              The Great Escape      Marillion    0.963130\n",
       "34448           Black Throated Wind  Grateful Dead    0.961648\n",
       "45512  Waiting For The Sirens' Call      New Order    0.961460"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = random.randint(0, len(df))\n",
    "test_song = df['song'].iloc[idx]\n",
    "test_artist = df['song'].iloc[idx]\n",
    "\n",
    "print(f\"Recommendations for: '{test_song}' by {test_artist}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nWord2Vec Recommendations:\")\n",
    "print(\"-\" * 40)\n",
    "display(recommend_songs_w2v(test_song))\n",
    "\n",
    "print(\"\\nFastText Recommendations:\")\n",
    "print(\"-\" * 40)\n",
    "display(recommend_songs_fasttext(test_song))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b26823",
   "metadata": {},
   "source": [
    "### 7.11 Save Semantic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85681fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: embeddings_fasttext.npz\n",
      "Saved: embeddings_w2v.npz\n",
      "Saved: faiss_w2v.index\n",
      "Saved: faiss_fasttext.index\n",
      "\n",
      "All semantic models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save FastText embeddings\n",
    "np.savez_compressed(\"../data/embeddings/embeddings_fasttext.npz\", X_ft)\n",
    "print(\"Saved: embeddings_fasttext.npz\")\n",
    "\n",
    "# Save Word2Vec embeddings (from pre-trained model)\n",
    "np.savez_compressed(\"../data/embeddings/embeddings_w2v.npz\", X_w2v)\n",
    "print(\"Saved: embeddings_w2v.npz\")\n",
    "\n",
    "# Save FAISS indexes\n",
    "faiss.write_index(index_w2v, \"../models/faiss_indexes/faiss_w2v.index\")\n",
    "print(\"Saved: faiss_w2v.index\")\n",
    "\n",
    "faiss.write_index(index_ft, \"../models/faiss_indexes/faiss_fasttext.index\")\n",
    "print(\"Saved: faiss_fasttext.index\")\n",
    "\n",
    "print(\"\\nAll semantic models saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f544bc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Improvement 2: Sentence-BERT\n",
    "\n",
    "> **Goal**: Use transformer-based embeddings that understand entire sentences/paragraphs, not just individual words.\n",
    "\n",
    "### Why Sentence-BERT?\n",
    "\n",
    "**Word2Vec Limitation vs SBERT Solution:**\n",
    "- Word2Vec averages word vectors (loses word order), SBERT encodes full sentence context\n",
    "- \"I love you\" = \"You love I\" in Word2Vec, but SBERT preserves word order meaning\n",
    "- Word2Vec has fixed vocabulary, SBERT handles any text\n",
    "\n",
    "### Model Options\n",
    "\n",
    "1. **`all-MiniLM-L6-v2`** (recommended)\n",
    "   - 384 dimensions\n",
    "   - Fast inference\n",
    "   - Good quality\n",
    "\n",
    "2. **`all-mpnet-base-v2`** (higher quality)\n",
    "   - 768 dimensions\n",
    "   - Slower but more accurate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10db11b6",
   "metadata": {},
   "source": [
    "### 8.1 Load Sentence-Transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5856574a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 256\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "sbert_model = SentenceTransformer(r'all-MiniLM-L6-v2')\n",
    "\n",
    "print(f\"Max sequence length: {sbert_model.max_seq_length}\")\n",
    "print(f\"Embedding dimension: {sbert_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e7ffa6",
   "metadata": {},
   "source": [
    "### 8.2 Generate SBERT Embeddings for All Songs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "501952ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f245a880c64549b1bfd2a7aeef1d73e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/901 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SBERT embeddings shape: (57650, 384)\n",
      "Memory usage: 84.45 MB\n"
     ]
    }
   ],
   "source": [
    "lyrics_texts = df['cleaned_text'].tolist()\n",
    "\n",
    "X_sbert = sbert_model.encode(\n",
    "    lyrics_texts,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=64,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "X_sbert = X_sbert.astype(np.float32)\n",
    "\n",
    "print(f\"\\nSBERT embeddings shape: {X_sbert.shape}\")\n",
    "print(f\"Memory usage: {X_sbert.nbytes / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2746efa",
   "metadata": {},
   "source": [
    "### 8.3 Build FAISS Index for SBERT Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d18e568a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FAISS index for SBERT embeddings...\n",
      "SBERT FAISS index: 57650 vectors, 384 dimensions\n"
     ]
    }
   ],
   "source": [
    "X_sbert_norm = normalize(X_sbert, norm='l2')\n",
    "\n",
    "print(\"Building FAISS index for SBERT embeddings...\")\n",
    "index_sbert = faiss.IndexFlatIP(X_sbert_norm.shape[1])\n",
    "index_sbert.add(X_sbert_norm)\n",
    "print(f\"SBERT FAISS index: {index_sbert.ntotal} vectors, {X_sbert_norm.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eccdd4e",
   "metadata": {},
   "source": [
    "### 8.4 SBERT Recommendation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01060692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_songs_sbert(song_name, df=df, top_k=5):\n",
    "    idx = df[df['song'].str.lower() == song_name.lower()].index\n",
    "    if len(idx) == 0:\n",
    "        return \"Song not found in the dataset\"\n",
    "    idx = idx[0]\n",
    "\n",
    "    query = X_sbert_norm[idx].reshape(1, -1)\n",
    "\n",
    "    scores, indices = index_sbert.search(query, top_k + 1)\n",
    "\n",
    "    result = df[['song', 'artist']].iloc[indices[0][1:]].copy()\n",
    "    result['similarity'] = scores[0][1:]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a0a8bf",
   "metadata": {},
   "source": [
    "### 8.5 Test the Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21dee4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for: 'Gangbang Rookie' by Snoop Dogg\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18609</th>\n",
       "      <td>Gangsta Ride</td>\n",
       "      <td>Snoop Dogg</td>\n",
       "      <td>0.836990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51826</th>\n",
       "      <td>Get Bout It Rowdy</td>\n",
       "      <td>Snoop Dogg</td>\n",
       "      <td>0.835201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51825</th>\n",
       "      <td>Gangsta Walk</td>\n",
       "      <td>Snoop Dogg</td>\n",
       "      <td>0.832067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41631</th>\n",
       "      <td>Don't Die</td>\n",
       "      <td>Lil Wayne</td>\n",
       "      <td>0.831523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46310</th>\n",
       "      <td>Player's Anthem</td>\n",
       "      <td>Notorious B.I.G.</td>\n",
       "      <td>0.826387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    song            artist  similarity\n",
       "18609       Gangsta Ride        Snoop Dogg    0.836990\n",
       "51826  Get Bout It Rowdy        Snoop Dogg    0.835201\n",
       "51825       Gangsta Walk        Snoop Dogg    0.832067\n",
       "41631          Don't Die         Lil Wayne    0.831523\n",
       "46310    Player's Anthem  Notorious B.I.G.    0.826387"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pick a random song for testing\n",
    "idx = random.randint(0, len(df) - 1)\n",
    "test_song = df['song'].iloc[idx]\n",
    "test_artist = df['artist'].iloc[idx]\n",
    "\n",
    "print(f\"Recommendations for: '{test_song}' by {test_artist}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "display(recommend_songs_sbert(test_song))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f1f9e",
   "metadata": {},
   "source": [
    "### 8.6 Save SBERT Models and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "baec9336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: embeddings_sbert.npz\n",
      "Saved: faiss_sbert.index\n",
      "\n",
      "SBERT models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save SBERT embeddings\n",
    "np.savez_compressed(\"../data/embeddings/embeddings_sbert.npz\", X_sbert)\n",
    "print(\"Saved: embeddings_sbert.npz\")\n",
    "\n",
    "# Save SBERT FAISS index\n",
    "faiss.write_index(index_sbert, \"../models/faiss_indexes/faiss_sbert.index\")\n",
    "print(\"Saved: faiss_sbert.index\")\n",
    "\n",
    "print(\"\\nSBERT models saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9c5943",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Improvement 3: Audio Features (Spotify API)\n",
    "\n",
    "> **Goal**: Incorporate audio characteristics (tempo, energy, danceability) to capture musical similarity beyond lyrics.\n",
    "\n",
    "### Spotify Audio Features\n",
    "\n",
    "- `danceability`: How suitable for dancing (0.0 - 1.0)\n",
    "- `energy`: Intensity and activity (0.0 - 1.0)\n",
    "- `tempo`: Beats per minute (~50 - 200)\n",
    "- `valence`: Musical positiveness (0.0 - 1.0)\n",
    "- `acousticness`: Acoustic vs electronic (0.0 - 1.0)\n",
    "- `instrumentalness`: Vocal vs instrumental (0.0 - 1.0)\n",
    "\n",
    "### Implementation Steps\n",
    "\n",
    "1. Set up Spotify API credentials\n",
    "2. Fetch audio features for each song\n",
    "3. Normalize and combine with lyrics embeddings\n",
    "4. Create hybrid similarity metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed75922",
   "metadata": {},
   "source": [
    "### 9.1 Set Up Spotify API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d48e1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Install spotipy and set up credentials\n",
    "# !pip install spotipy -q\n",
    "# import spotipy\n",
    "# from spotipy.oauth2 import SpotifyClientCredentials\n",
    "# sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id=\"...\", client_secret=\"...\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139600f4",
   "metadata": {},
   "source": [
    "### 9.2 Fetch Audio Features for Songs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "863e499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Search for tracks and get audio features\n",
    "# def get_audio_features(artist, song):\n",
    "#     results = sp.search(q=f\"artist:{artist} track:{song}\", type='track', limit=1)\n",
    "#     if results['tracks']['items']:\n",
    "#         track_id = results['tracks']['items'][0]['id']\n",
    "#         return sp.audio_features(track_id)[0]\n",
    "#     return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece037b",
   "metadata": {},
   "source": [
    "### 9.3 Create Audio Feature Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "608e2430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalize and combine audio features into embeddings\n",
    "# audio_features = ['danceability', 'energy', 'tempo', 'valence', 'acousticness', 'instrumentalness']\n",
    "# X_audio = df[audio_features].values\n",
    "# X_audio = normalize(X_audio, norm='l2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46049175",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Embedding Comparison & Evaluation\n",
    "\n",
    "> **Goal**: Compare TF-IDF, Word2Vec, and SBERT embeddings for lyric-based song similarity. Evaluate which method produces the most meaningful recommendations.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "- **Qualitative**: Manual inspection of recommendation quality\n",
    "- **Diversity**: How diverse are the recommended songs?\n",
    "- **Artist Coverage**: Do recommendations include different artists?\n",
    "- **Semantic Coherence**: Do lyrics share similar themes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002de6fb",
   "metadata": {},
   "source": [
    "### 10.1 Side-by-Side Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03f2dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare recommendations from all methods for the same song\n",
    "# def compare_recommendations(song_name, top_k=5):\n",
    "#     print(f\"Recommendations for: {song_name}\")\n",
    "#     print(\"\\n📊 TF-IDF:\")\n",
    "#     display(recommend_songs_tfidf(song_name, top_k))\n",
    "#     print(\"\\n📊 Word2Vec:\")\n",
    "#     display(recommend_songs_w2v(song_name, top_k))\n",
    "#     print(\"\\n📊 Sentence-BERT:\")\n",
    "#     display(recommend_songs_sbert(song_name, top_k))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a3d295",
   "metadata": {},
   "source": [
    "### 10.2 Embedding Visualization (t-SNE/UMAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1207f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize embeddings in 2D using t-SNE or UMAP\n",
    "# from sklearn.manifold import TSNE\n",
    "# X_embedded = TSNE(n_components=2).fit_transform(X_sbert[:1000])\n",
    "# plt.scatter(X_embedded[:, 0], X_embedded[:, 1], alpha=0.5)\n",
    "# plt.title(\"SBERT Embeddings (t-SNE)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e2fc1",
   "metadata": {},
   "source": [
    "### 10.3 Summary Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2bc30c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create summary comparison table\n",
    "# comparison_df = pd.DataFrame({\n",
    "#     'Method': ['TF-IDF', 'Word2Vec', 'FastText', 'SBERT'],\n",
    "#     'Dimensions': [5000, 300, 100, 384],\n",
    "#     'Semantic': ['❌', '✅', '✅', '✅✅'],\n",
    "#     'Speed': ['Fast', 'Fast', 'Fast', 'Slow'],\n",
    "#     'Quality': ['Good', 'Better', 'Better', 'Best']\n",
    "# })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf46bd2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Hybrid Recommendation System\n",
    "\n",
    "> **Goal**: Combine multiple similarity signals (lyrics, genre, artist, popularity) for better recommendations.\n",
    "\n",
    "### Hybrid Score Formula\n",
    "\n",
    "$$\\text{FinalScore} = w_1 \\cdot \\text{LyricSim} + w_2 \\cdot \\text{GenreSim} + w_3 \\cdot \\text{Popularity}$$\n",
    "\n",
    "**Default Weights:**\n",
    "- w1 = 0.5 (Lyrics similarity - most important)\n",
    "- w2 = 0.3 (Genre/Artist similarity)\n",
    "- w3 = 0.2 (Popularity score)\n",
    "\n",
    "### Components to Combine\n",
    "\n",
    "- **Lyrics Similarity**: SBERT/TF-IDF cosine similarity (0.5)\n",
    "- **Artist Similarity**: Same/similar artist bonus (0.1)\n",
    "- **Genre Similarity**: Same genre bonus (0.2)\n",
    "- **Popularity**: Normalized play count (0.1)\n",
    "- **Release Year**: Recency bonus (0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c042b3e1",
   "metadata": {},
   "source": [
    "### 11.1 Lyrics Similarity Component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c61acc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get lyrics similarity scores from SBERT index\n",
    "# def get_lyrics_similarity(song_idx, candidate_indices):\n",
    "#     query = X_sbert[song_idx].reshape(1, -1)\n",
    "#     scores, _ = index_sbert.search(query, len(candidate_indices))\n",
    "#     return scores[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131b4c53",
   "metadata": {},
   "source": [
    "### 11.2 Artist / Genre Similarity Component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa06b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute artist/genre similarity\n",
    "# def get_artist_similarity(song_idx, candidate_indices):\n",
    "#     query_artist = df.iloc[song_idx]['artist']\n",
    "#     similarities = []\n",
    "#     for idx in candidate_indices:\n",
    "#         if df.iloc[idx]['artist'] == query_artist:\n",
    "#             similarities.append(1.0)  # Same artist\n",
    "#         else:\n",
    "#             similarities.append(0.0)\n",
    "#     return np.array(similarities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62342ea0",
   "metadata": {},
   "source": [
    "### 11.3 Popularity / Release Year Component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "576f20b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute popularity scores (if available)\n",
    "# def get_popularity_scores(candidate_indices):\n",
    "#     if 'popularity' in df.columns:\n",
    "#         scores = df.iloc[candidate_indices]['popularity'].values\n",
    "#         return (scores - scores.min()) / (scores.max() - scores.min())\n",
    "#     return np.zeros(len(candidate_indices))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ece7d3",
   "metadata": {},
   "source": [
    "### 11.4 Hybrid Recommendation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a9e504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement hybrid recommendation\n",
    "# def recommend_songs_hybrid(song_name, top_k=5, weights={'lyrics': 0.5, 'genre': 0.3, 'popularity': 0.2}):\n",
    "#     idx = df[df['song'].str.lower() == song_name.lower()].index[0]\n",
    "#     \n",
    "#     # Get initial candidates from lyrics similarity\n",
    "#     _, candidates = index_sbert.search(X_sbert[idx].reshape(1, -1), 100)\n",
    "#     candidates = candidates[0][1:]  # Exclude query song\n",
    "#     \n",
    "#     # Compute component scores\n",
    "#     lyrics_scores = get_lyrics_similarity(idx, candidates)\n",
    "#     artist_scores = get_artist_similarity(idx, candidates)\n",
    "#     popularity_scores = get_popularity_scores(candidates)\n",
    "#     \n",
    "#     # Combine with weights\n",
    "#     final_scores = (weights['lyrics'] * lyrics_scores + \n",
    "#                     weights['genre'] * artist_scores + \n",
    "#                     weights['popularity'] * popularity_scores)\n",
    "#     \n",
    "#     # Return top-k\n",
    "#     top_indices = np.argsort(final_scores)[::-1][:top_k]\n",
    "#     return df.iloc[candidates[top_indices]][['song', 'artist']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413c6b27",
   "metadata": {},
   "source": [
    "### 11.5 Weight Tuning & Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de406557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test different weight combinations\n",
    "# weight_configs = [\n",
    "#     {'lyrics': 0.5, 'genre': 0.3, 'popularity': 0.2},\n",
    "#     {'lyrics': 0.7, 'genre': 0.2, 'popularity': 0.1},\n",
    "#     {'lyrics': 0.4, 'genre': 0.4, 'popularity': 0.2},\n",
    "# ]\n",
    "# \n",
    "# for config in weight_configs:\n",
    "#     print(f\"Weights: {config}\")\n",
    "#     display(recommend_songs_hybrid(test_song, weights=config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf36be6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Advanced FAISS Indexes\n",
    "\n",
    "> **Goal**: Compare different FAISS index types for speed vs accuracy trade-offs.\n",
    "\n",
    "### Index Types Comparison\n",
    "\n",
    "- **IndexFlatIP**: Exact search (baseline) - Slow, High Memory, 100% Recall\n",
    "- **IndexIVFFlat**: Inverted file index - Fast, Medium Memory, ~95% Recall\n",
    "- **IndexHNSWFlat**: Hierarchical NSW graph - Very Fast, Medium Memory, ~98% Recall\n",
    "- **IndexIVFPQ**: Product quantization - Very Fast, Low Memory, ~90% Recall\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "- **IndexFlatIP**: Small datasets (<100K), need exact results\n",
    "- **IndexIVFFlat**: Medium datasets, good balance\n",
    "- **IndexHNSWFlat**: Large datasets, best quality/speed\n",
    "- **IndexIVFPQ**: Very large datasets, memory constrained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f4a824",
   "metadata": {},
   "source": [
    "### 12.1 Baseline: IndexFlatIP (Current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60fb7564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: Exact search\n",
    "# index_flat = faiss.IndexFlatIP(dim)\n",
    "# index_flat.add(X)\n",
    "# print(f\"IndexFlatIP: {index_flat.ntotal} vectors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61287b6",
   "metadata": {},
   "source": [
    "### 12.2 IndexIVFFlat (Inverted File Index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98a9024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build IndexIVFFlat\n",
    "# nlist = 100  # Number of clusters\n",
    "# quantizer = faiss.IndexFlatIP(dim)\n",
    "# index_ivf = faiss.IndexIVFFlat(quantizer, dim, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "# index_ivf.train(X)\n",
    "# index_ivf.add(X)\n",
    "# index_ivf.nprobe = 10  # Number of clusters to search\n",
    "# print(f\"IndexIVFFlat: {index_ivf.ntotal} vectors, {nlist} clusters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b0d0de",
   "metadata": {},
   "source": [
    "### 12.3 IndexHNSWFlat (Hierarchical NSW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "065ffb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build IndexHNSWFlat\n",
    "# M = 32  # Number of connections per layer\n",
    "# index_hnsw = faiss.IndexHNSWFlat(dim, M, faiss.METRIC_INNER_PRODUCT)\n",
    "# index_hnsw.hnsw.efConstruction = 40  # Construction-time parameter\n",
    "# index_hnsw.hnsw.efSearch = 16  # Search-time parameter\n",
    "# index_hnsw.add(X)\n",
    "# print(f\"IndexHNSWFlat: {index_hnsw.ntotal} vectors, M={M}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6b3ff4",
   "metadata": {},
   "source": [
    "### 12.4 Speed Comparison Benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23626bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Benchmark search speed\n",
    "# import time\n",
    "# \n",
    "# def benchmark_index(index, name, X, n_queries=100, top_k=10):\n",
    "#     queries = X[np.random.choice(len(X), n_queries)]\n",
    "#     start = time.time()\n",
    "#     for q in queries:\n",
    "#         index.search(q.reshape(1, -1), top_k)\n",
    "#     elapsed = time.time() - start\n",
    "#     qps = n_queries / elapsed\n",
    "#     print(f\"{name}: {qps:.1f} queries/sec ({elapsed*1000/n_queries:.2f} ms/query)\")\n",
    "#     return qps\n",
    "# \n",
    "# benchmark_index(index_flat, \"IndexFlatIP\", X)\n",
    "# benchmark_index(index_ivf, \"IndexIVFFlat\", X)\n",
    "# benchmark_index(index_hnsw, \"IndexHNSWFlat\", X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54824aad",
   "metadata": {},
   "source": [
    "### 12.5 Memory Usage Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04f0ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare memory usage\n",
    "# import sys\n",
    "# \n",
    "# def get_index_size(index):\n",
    "#     faiss.write_index(index, \"/tmp/temp_index\")\n",
    "#     size_mb = os.path.getsize(\"/tmp/temp_index\") / 1024 / 1024\n",
    "#     return size_mb\n",
    "# \n",
    "# print(f\"IndexFlatIP: {get_index_size(index_flat):.2f} MB\")\n",
    "# print(f\"IndexIVFFlat: {get_index_size(index_ivf):.2f} MB\")\n",
    "# print(f\"IndexHNSWFlat: {get_index_size(index_hnsw):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b104c4",
   "metadata": {},
   "source": [
    "### 12.6 Recall@K Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "85e35a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute Recall@K (how many of exact top-K are found by ANN)\n",
    "# def compute_recall(index_exact, index_approx, X, n_queries=100, k=10):\n",
    "#     queries = X[np.random.choice(len(X), n_queries)]\n",
    "#     recalls = []\n",
    "#     for q in queries:\n",
    "#         q = q.reshape(1, -1)\n",
    "#         _, exact_ids = index_exact.search(q, k)\n",
    "#         _, approx_ids = index_approx.search(q, k)\n",
    "#         recall = len(set(exact_ids[0]) & set(approx_ids[0])) / k\n",
    "#         recalls.append(recall)\n",
    "#     return np.mean(recalls)\n",
    "# \n",
    "# print(f\"IndexIVFFlat Recall@10: {compute_recall(index_flat, index_ivf, X):.2%}\")\n",
    "# print(f\"IndexHNSWFlat Recall@10: {compute_recall(index_flat, index_hnsw, X):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad7bd8",
   "metadata": {},
   "source": [
    "### 12.7 Performance Comparison Chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c803ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create visualization comparing all indexes\n",
    "# results = pd.DataFrame({\n",
    "#     'Index': ['IndexFlatIP', 'IndexIVFFlat', 'IndexHNSWFlat'],\n",
    "#     'Speed (QPS)': [100, 1000, 5000],\n",
    "#     'Memory (MB)': [500, 550, 600],\n",
    "#     'Recall@10': [1.0, 0.95, 0.98]\n",
    "# })\n",
    "# \n",
    "# fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "# results.plot.bar(x='Index', y='Speed (QPS)', ax=axes[0], color='steelblue')\n",
    "# results.plot.bar(x='Index', y='Memory (MB)', ax=axes[1], color='coral')\n",
    "# results.plot.bar(x='Index', y='Recall@10', ax=axes[2], color='seagreen')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

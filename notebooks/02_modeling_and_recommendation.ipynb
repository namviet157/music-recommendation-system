{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4d23924",
   "metadata": {},
   "source": [
    "# Music Recommendation System\n",
    "## Part 2: Modeling & Recommendation Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8107be",
   "metadata": {},
   "source": [
    "This notebook implements **various embedding techniques and recommendation strategies** for our content-based music recommendation system. We compare TF-IDF, Word2Vec, and Sentence-BERT embeddings for lyric-based song similarity.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "### Core Implementation\n",
    "1. [Import Libraries](#1-import-libraries)\n",
    "2. [Load Dataset](#2-load-dataset)\n",
    "3. [Baseline: TF-IDF](#3-baseline-tf-idf)\n",
    "4. [Building FAISS Index](#4-building-faiss-index-baseline)\n",
    "5. [Recommendation Function](#5-recommendation-function)\n",
    "6. [Model Persistence](#6-model-persistence)\n",
    "\n",
    "### Improvements\n",
    "\n",
    "#### Upgrade Song Embeddings\n",
    "7. [Word2Vec / FastText](#7-improvement-1-word2vec--fasttext)\n",
    "8. [Sentence-BERT](#8-improvement-2-sentence-bert)\n",
    "9. [Audio Features (Spotify API)](#9-improvement-3-audio-features-spotify-api)\n",
    "10. [Embedding Comparison](#10-embedding-comparison--evaluation)\n",
    "\n",
    "#### Hybrid Recommendation\n",
    "11. [Hybrid Recommendation System](#11-hybrid-recommendation-system)\n",
    "\n",
    "#### Advanced FAISS (ANN)\n",
    "12. [Advanced FAISS Indexes](#12-advanced-faiss-indexes)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb61039",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3817588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import normalize\n",
    "import faiss\n",
    "import pickle\n",
    "from scipy.sparse import save_npz\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import HfApi, login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7689de0f",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "243b8841",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/spotify_millsongdata_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b666ddb",
   "metadata": {},
   "source": [
    "## 3. Baseline: TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0cf621",
   "metadata": {},
   "source": [
    "> **Baseline Approach**: We start with TF-IDF (Term Frequency-Inverse Document Frequency) as our baseline embedding method. This is a classic approach for text similarity that works well but has limitations in capturing semantic meaning.\n",
    "\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency)** is a numerical statistic that reflects how important a word is to a document in a collection.\n",
    "\n",
    "- **Term Frequency (TF)**: How often a word appears in a document\n",
    "\n",
    "$$\n",
    "tf(t, d) = \\frac{f_{t,d}}{\\sum_{k} f_{k,d}}\n",
    "$$\n",
    "\n",
    "where $f_{t,d}$ denotes the number of occurrences of term $t$ in document $d$.\n",
    "\n",
    "- **Inverse Document Frequency (IDF)**: How rare a word is across all documents\n",
    "\n",
    "$$\n",
    "idf(t, D) = \\log \\left( \\frac{N}{1 + n_t} \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $N$ is the total number of documents in the corpus,\n",
    "- $n_t$ is the number of documents containing term $t$.\n",
    "\n",
    "\n",
    "**TF-IDF Weight**\n",
    "\n",
    "The final TF-IDF weight of term $t$ in document $d$ is computed as:\n",
    "\n",
    "$$\n",
    "w_{t,d} = tf(t, d) \\times idf(t, D)\n",
    "$$\n",
    "\n",
    "Words that appear frequently in one song but rarely across all songs get higher weights, making them better for distinguishing between songs.\n",
    "\n",
    "We limit to `max_features=5000` to keep the most important terms and reduce dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ffa898b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix Shape: (57650, 5000)\n",
      "Vocabulary Size: 5000\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, dtype=np.float32)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
    "\n",
    "print(f\"TF-IDF Matrix Shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Vocabulary Size: {len(tfidf_vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a54a54c",
   "metadata": {},
   "source": [
    "## 4. Building FAISS Index (Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b0a9c9",
   "metadata": {},
   "source": [
    "**FAISS (Facebook AI Similarity Search)** is a library for efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "### **Why FAISS?**\n",
    "- Traditional cosine similarity has O(n) complexity for each query\n",
    "- FAISS uses optimized algorithms for faster nearest neighbor search\n",
    "- Essential for scaling to large datasets (millions of songs)\n",
    "\n",
    "---\n",
    "\n",
    "### **Index Type: `IndexFlatIP`**\n",
    "- **IP** = Inner Product (equivalent to cosine similarity for L2-normalized vectors)\n",
    "- **Flat** = Exact search (no approximation)\n",
    "- We normalize vectors with L2 norm so inner product equals cosine similarity\n",
    "\n",
    "#### **$L_2$ Normalization**\n",
    "\n",
    "$L_2$ normalization transforms a document vector  $\\mathbf{x} = [x_1, x_2, \\dots, x_n]$  into a unit vector $\\hat{\\mathbf{x}}$ with Euclidean norm equal to 1:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}} = \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2}\n",
    "= \\frac{\\mathbf{x}}{\\sqrt{\\sum_{i=1}^{n} x_i^2}}\n",
    "$$\n",
    "\n",
    "##### **Geometric Interpretation**\n",
    "\n",
    "After $L_2$ normalization, all document vectors lie on the surface of a **unit hypersphere**. This normalization removes the influence of document length, ensuring that similarity comparisons depend only on the **direction** of the vectors (semantic content), rather than their magnitude.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Similarity Search with FAISS and Inner Product**\n",
    "\n",
    "In FAISS, `IndexFlatIP` performs similarity search based on the **Inner Product (IP)**.  \n",
    "Given a query vector $\\mathbf{q}$ and a set of document vectors $\\mathbf{X}$ stored in the index, the similarity score $s_i$ is computed as:\n",
    "\n",
    "$$\n",
    "s_i = \\mathbf{q} \\cdot \\mathbf{x}_i\n",
    "= \\sum_{j=1}^{dim} q_j \\cdot x_{i,j}\n",
    "$$\n",
    "\n",
    "##### **Relationship Between Inner Product and Cosine Similarity**\n",
    "\n",
    "Cosine Similarity between two vectors $\\mathbf{q}$ and $\\mathbf{x}$ is defined as:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_sim}(\\mathbf{q}, \\mathbf{x})\n",
    "= \\frac{\\mathbf{q} \\cdot \\mathbf{x}}{\\|\\mathbf{q}\\|_2 \\, \\|\\mathbf{x}\\|_2}\n",
    "$$\n",
    "\n",
    "Since both the query vector and document vectors are $L_2$-normalized  ($\\|\\mathbf{q}\\|_2 = 1$ and $\\|\\mathbf{x}\\|_2 = 1$), the equation simplifies to:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_sim}(\\mathbf{q}, \\mathbf{x})\n",
    "= \\mathbf{q} \\cdot \\mathbf{x}\n",
    "= \\text{Inner Product}\n",
    "$$\n",
    "\n",
    "Therefore, when using `IndexFlatIP` on $L_2$-normalized TF-IDF vectors, FAISS effectively computes **Cosine Similarity**, representing the degree of semantic similarity between the query text and the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc72d6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Dimension: 5000\n",
      "Total vectors in FAISS index: 57650\n",
      "FAISS index built successfully!\n"
     ]
    }
   ],
   "source": [
    "X = tfidf_matrix.toarray()\n",
    "X = normalize(X, norm=\"l2\")\n",
    "\n",
    "dim = X.shape[1]\n",
    "print(f\"Vector Dimension: {dim}\")\n",
    "\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(X)\n",
    "\n",
    "print(f\"Total vectors in FAISS index: {index.ntotal}\")\n",
    "print(\"FAISS index built successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd46207",
   "metadata": {},
   "source": [
    "## 5. Recommendation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b842852c",
   "metadata": {},
   "source": [
    "The core recommendation logic:\n",
    "\n",
    "1. **Find the song**: Look up the input song name in our dataset\n",
    "2. **Get its vector**: Retrieve the TF-IDF vector for that song\n",
    "3. **Search similar songs**: Use FAISS to find the k most similar songs\n",
    "4. **Return results**: Return song details (excluding the input song itself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a18d0344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_songs_tfidf(song_name, df=df, top_k=5):\n",
    "\n",
    "    idx = df[df['song'].str.lower() == song_name.lower()].index\n",
    "    if len(idx) == 0:\n",
    "        return \"Song not found in the dataset.\"\n",
    "    idx = idx[0]\n",
    "\n",
    "    query = X[idx].reshape(1, -1)\n",
    "\n",
    "    scores, indices = index.search(query, top_k + 1)\n",
    "\n",
    "    results = df[['song', 'artist']].iloc[indices[0][1:]].copy()\n",
    "    results['similarity'] = scores[0][1:]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81acee30",
   "metadata": {},
   "source": [
    "### Test the Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10d9651",
   "metadata": {},
   "source": [
    "Let's test our recommendation system by getting similar songs for the first song in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e0a9338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting recommendations for: 'Girls It Ain't Easy' by Dusty Springfield\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11631</th>\n",
       "      <td>It's So Easy (To Fall In Love)</td>\n",
       "      <td>Linda Ronstadt</td>\n",
       "      <td>0.669506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40634</th>\n",
       "      <td>Easy As It Seems</td>\n",
       "      <td>Kiss</td>\n",
       "      <td>0.637069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11877</th>\n",
       "      <td>Easy</td>\n",
       "      <td>Lorde</td>\n",
       "      <td>0.632390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4904</th>\n",
       "      <td>It Ain't Gonna Be Easy</td>\n",
       "      <td>Elton John</td>\n",
       "      <td>0.605216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41729</th>\n",
       "      <td>It's So Easy</td>\n",
       "      <td>Linda Ronstadt</td>\n",
       "      <td>0.596836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 song          artist  similarity\n",
       "11631  It's So Easy (To Fall In Love)  Linda Ronstadt    0.669506\n",
       "40634                Easy As It Seems            Kiss    0.637069\n",
       "11877                            Easy           Lorde    0.632390\n",
       "4904           It Ain't Gonna Be Easy      Elton John    0.605216\n",
       "41729                    It's So Easy  Linda Ronstadt    0.596836"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = random.randint(0, len(df))\n",
    "test_song = df['song'].iloc[idx]\n",
    "test_artist = df['artist'].iloc[idx]\n",
    "print(f\"Getting recommendations for: '{test_song}' by {test_artist}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "display(recommend_songs_tfidf(test_song))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0840d7a",
   "metadata": {},
   "source": [
    "## 6. Model Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dd1479",
   "metadata": {},
   "source": [
    "Save the trained models and processed data for later use:\n",
    "\n",
    "- **df_cleaned.pkl**: Preprocessed DataFrame with cleaned lyrics\n",
    "- **faiss_index.pkl**: FAISS index for fast similarity search\n",
    "- **tfidf_matrix.pkl**: TF-IDF vectors for all songs\n",
    "\n",
    "These files can be loaded later to make recommendations without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60cd5729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: df_cleaned.parquet\n",
      "Saved: embeddings_tfidf.npz\n",
      "Saved: faiss_tfidf.index\n",
      "\n",
      "All models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "df.to_parquet(\"../data/processed/df_cleaned.parquet\", index=False)\n",
    "print(\"Saved: df_cleaned.parquet\")\n",
    "\n",
    "save_npz(\"../data/embeddings/embeddings_tfidf.npz\", tfidf_matrix)\n",
    "print(\"Saved: embeddings_tfidf.npz\")\n",
    "\n",
    "faiss.write_index(index, \"../models/faiss_indexes/faiss_tfidf.index\")\n",
    "print(\"Saved: faiss_tfidf.index\")\n",
    "\n",
    "print(\"\\nAll models saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f45ff0b",
   "metadata": {},
   "source": [
    "---\n",
    "# IMPROVEMENTS\n",
    "\n",
    "The following sections implement advanced techniques to improve recommendation quality and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f80cc3b",
   "metadata": {},
   "source": [
    "## 7. Improvement 1: Word2Vec / FastText\n",
    "\n",
    "> **Goal**: Capture semantic similarity between words. Unlike TF-IDF which treats words as independent tokens, word embeddings represent words in a continuous vector space where semantically similar words are close together.\n",
    "\n",
    "### Why Word Embeddings?\n",
    "\n",
    "**Limitation of TF-IDF vs Solution with Word2Vec/FastText:**\n",
    "- \"happy\" and \"joyful\" are completely different features in TF-IDF, but similar vectors in Word2Vec\n",
    "- TF-IDF has no semantic understanding, Word2Vec captures word meaning\n",
    "- OOV words are ignored in TF-IDF, FastText handles via subwords\n",
    "\n",
    "### Approaches\n",
    "\n",
    "1. **Pre-trained Word2Vec** (Google News 300d)\n",
    "   - 3M words, 300 dimensions\n",
    "   - General semantic knowledge\n",
    "   \n",
    "2. **Custom FastText** (trained on lyrics)\n",
    "   - Domain-specific vocabulary\n",
    "   - Handles misspellings and slang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df00ecac",
   "metadata": {},
   "source": [
    "### 7.1 Tokenize Lyrics for Word Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e851e1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\r\\nA...</td>\n",
       "      <td>[look, face, wonderful, face, means, something...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>Take it easy with me, please  \\r\\nTouch me gen...</td>\n",
       "      <td>[take, easy, please, touch, gently, like, summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As Good As New</td>\n",
       "      <td>I'll never know why I had to go  \\r\\nWhy I had...</td>\n",
       "      <td>[ill, never, know, go, put, lousy, rotten, sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bang</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>[making, somebody, happy, question, give, take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>[making, somebody, happy, question, give, take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57645</th>\n",
       "      <td>Good Old Days</td>\n",
       "      <td>Irie days come on play  \\r\\nLet the angels fly...</td>\n",
       "      <td>[irie, days, come, play, let, angels, fly, let...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57646</th>\n",
       "      <td>Hand To Mouth</td>\n",
       "      <td>Power to the workers  \\r\\nMore power  \\r\\nPowe...</td>\n",
       "      <td>[power, workers, power, power, workers, need, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57647</th>\n",
       "      <td>Come With Me</td>\n",
       "      <td>all you need  \\r\\nis something i'll believe  \\...</td>\n",
       "      <td>[need, something, ill, believe, flashlights, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57648</th>\n",
       "      <td>Desire</td>\n",
       "      <td>northern star  \\r\\nam i frightened  \\r\\nwhere ...</td>\n",
       "      <td>[northern, star, frightened, go, rest, cant, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57649</th>\n",
       "      <td>Heartsong</td>\n",
       "      <td>come in  \\r\\nmake yourself at home  \\r\\ni'm a ...</td>\n",
       "      <td>[come, make, home, im, bit, late, hate, make, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57650 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        song  \\\n",
       "0      Ahe's My Kind Of Girl   \n",
       "1           Andante, Andante   \n",
       "2             As Good As New   \n",
       "3                       Bang   \n",
       "4           Bang-A-Boomerang   \n",
       "...                      ...   \n",
       "57645          Good Old Days   \n",
       "57646          Hand To Mouth   \n",
       "57647           Come With Me   \n",
       "57648                 Desire   \n",
       "57649              Heartsong   \n",
       "\n",
       "                                                    text  \\\n",
       "0      Look at her face, it's a wonderful face  \\r\\nA...   \n",
       "1      Take it easy with me, please  \\r\\nTouch me gen...   \n",
       "2      I'll never know why I had to go  \\r\\nWhy I had...   \n",
       "3      Making somebody happy is a question of give an...   \n",
       "4      Making somebody happy is a question of give an...   \n",
       "...                                                  ...   \n",
       "57645  Irie days come on play  \\r\\nLet the angels fly...   \n",
       "57646  Power to the workers  \\r\\nMore power  \\r\\nPowe...   \n",
       "57647  all you need  \\r\\nis something i'll believe  \\...   \n",
       "57648  northern star  \\r\\nam i frightened  \\r\\nwhere ...   \n",
       "57649  come in  \\r\\nmake yourself at home  \\r\\ni'm a ...   \n",
       "\n",
       "                                                  tokens  \n",
       "0      [look, face, wonderful, face, means, something...  \n",
       "1      [take, easy, please, touch, gently, like, summ...  \n",
       "2      [ill, never, know, go, put, lousy, rotten, sho...  \n",
       "3      [making, somebody, happy, question, give, take...  \n",
       "4      [making, somebody, happy, question, give, take...  \n",
       "...                                                  ...  \n",
       "57645  [irie, days, come, play, let, angels, fly, let...  \n",
       "57646  [power, workers, power, power, workers, need, ...  \n",
       "57647  [need, something, ill, believe, flashlights, h...  \n",
       "57648  [northern, star, frightened, go, rest, cant, s...  \n",
       "57649  [come, make, home, im, bit, late, hate, make, ...  \n",
       "\n",
       "[57650 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'] = df['cleaned_text'].apply(lambda x: x.split())\n",
    "\n",
    "df[['song', 'text', 'tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc09e743",
   "metadata": {},
   "source": [
    "### 7.2 Option A: Using Pre-trained Word2Vec (Google News)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77b949f",
   "metadata": {},
   "source": [
    "Using pre-trained Word2Vec model trained on Google News (3 million words, 300-dimensional vectors).\n",
    "\n",
    "**Pros:** \n",
    "- High-quality embeddings trained on massive corpus\n",
    "- Captures general semantic relationships\n",
    "\n",
    "**Cons:**\n",
    "- Large download (~1.5GB)\n",
    "- May miss domain-specific music vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e01f3287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3,000,000\n",
      "Vector dimension: 300\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    w2v_model = api.load('word2vec-google-news-300')\n",
    "    print(f\"Vocabulary size: {len(w2v_model.key_to_index):,}\")\n",
    "    print(f\"Vector dimension: {w2v_model.vector_size}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45b0c17",
   "metadata": {},
   "source": [
    "### 7.3 Document Embedding Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12337ac3",
   "metadata": {},
   "source": [
    "To get a single vector representation for an entire song (document), we compute the **weighted average** of word vectors:\n",
    "\n",
    "$$\\vec{d} = \\frac{1}{|W|} \\sum_{w \\in W} \\vec{w}$$\n",
    "\n",
    "where $W$ is the set of words in the document and $\\vec{w}$ is the word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7967f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_embedding(tokens, model, vector_size=300):\n",
    "    \"\"\"\n",
    "    Compute document embedding by averaging word vectors.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of words in the document\n",
    "        model: Word2Vec/FastText model\n",
    "        vector_size: Dimension of word vectors\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (vector_size,) - document embedding\n",
    "    \"\"\"\n",
    "\n",
    "    word_vectors = []\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            word_vectors.append(model[word])\n",
    "        except KeyError:\n",
    "            continue\n",
    "     \n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zero(vector_size)\n",
    "    \n",
    "    return np.mean(word_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a567b1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document embedding shape: (300,)\n",
      "First 10 values: [ 0.03464193  0.01799033  0.04964281  0.09320665 -0.0741931  -0.0054287\n",
      "  0.05288998 -0.10220951  0.0597229   0.04583878]\n"
     ]
    }
   ],
   "source": [
    "sample_tokens = df['tokens'].iloc[0]\n",
    "sample_embedding = get_document_embedding(sample_tokens, w2v_model)\n",
    "print(f\"Sample document embedding shape: {sample_embedding.shape}\")\n",
    "print(f\"First 10 values: {sample_embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948da8b8",
   "metadata": {},
   "source": [
    "### 7.4 Build Document Embeddings for All Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5ebcad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing songs: 100%|██████████| 57650/57650 [00:08<00:00, 7089.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document embeddings shape: (57650, 300)\n",
      "Memory usage: 65.98 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_embedding_w2v = []\n",
    "for tokens in tqdm(df['tokens'], desc=\"Processing songs\"):\n",
    "    embedding = get_document_embedding(tokens, w2v_model)\n",
    "    doc_embedding_w2v.append(embedding)\n",
    "\n",
    "X_w2v = np.array(doc_embedding_w2v, dtype=np.float32)\n",
    "\n",
    "print(f\"Document embeddings shape: {X_w2v.shape}\")\n",
    "print(f\"Memory usage: {X_w2v.nbytes / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b650cbf3",
   "metadata": {},
   "source": [
    "### 7.5 Option B: Train Custom FastText Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f80b72",
   "metadata": {},
   "source": [
    "FastText extends Word2Vec by representing words as bags of character n-grams. This allows it to:\n",
    "- Handle **out-of-vocabulary (OOV)** words by composing subword vectors\n",
    "- Better handle **misspellings** and **slang** common in lyrics\n",
    "- Capture **morphological** relationships (e.g., \"love\", \"loving\", \"loved\")\n",
    "\n",
    "**Training Parameters:**\n",
    "- `vector_size=100`: Dimension of word vectors (smaller for faster training)\n",
    "- `window=5`: Context window size\n",
    "- `min_count=2`: Ignore words that appear less than 2 times\n",
    "- `epochs=10`: Number of training iterations\n",
    "- `sg=1`: Use Skip-gram (better for small datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f75235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText model trained successfully!\n",
      "Vocabulary size: 52,953\n",
      "Vector dimension: 100\n"
     ]
    }
   ],
   "source": [
    "fasttext_model = FastText(\n",
    "    sentences=df['tokens'].to_list(),\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "print(f\"FastText model trained successfully!\")\n",
    "print(f\"Vocabulary size: {len(fasttext_model.wv.key_to_index):,}\")\n",
    "print(f\"Vector dimension: {fasttext_model.wv.vector_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a1ecf5",
   "metadata": {},
   "source": [
    "### 7.6 Explore Semantic Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d071691a",
   "metadata": {},
   "source": [
    "Let's verify that our FastText model captured meaningful semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "845c6890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to common lyrics terms:\n",
      "\n",
      "'love' → mylove (0.785), alove (0.782), ohlove (0.779), itlove (0.769), lovelove (0.758)\n",
      "'heart' → heartfelt (0.803), heartmy (0.787), heartheart (0.763), hearts (0.742), hearttheres (0.726)\n",
      "'dance' → dancego (0.800), decadance (0.759), dancefloor (0.742), danceevery (0.725), dancins (0.714)\n",
      "'night' → nightgo (0.801), nightynight (0.791), nightoh (0.782), nightspot (0.769), nightnight (0.763)\n",
      "'baby' → ohbaby (0.813), ofbaby (0.803), bbaby (0.798), saidbaby (0.788), babyi (0.783)\n"
     ]
    }
   ],
   "source": [
    "test_words = ['love', 'heart', 'dance', 'night', 'baby']\n",
    "\n",
    "print(\"Words most similar to common lyrics terms:\\n\")\n",
    "for word in test_words:\n",
    "    try:\n",
    "        similar = fasttext_model.wv.most_similar(word, topn=5)\n",
    "        similar_words = [f\"{w} ({s:.3f})\" for w, s in similar]\n",
    "        print(f\"'{word}' → {', '.join(similar_words)}\")\n",
    "    except KeyError:\n",
    "        print(f\"'{word}' not in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e1376",
   "metadata": {},
   "source": [
    "### 7.7 Build FastText Document Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e990fd17",
   "metadata": {},
   "source": [
    "Now let's create document embeddings using our trained FastText model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7493765a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing FastText document embeddings for all songs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing songs: 100%|██████████| 57650/57650 [00:09<00:00, 5975.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FastText document embeddings shape: (57650, 100)\n",
      "Memory usage: 21.99 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing FastText document embeddings for all songs...\")\n",
    "\n",
    "doc_embeddings_ft = []\n",
    "for tokens in tqdm(df['tokens'], desc=\"Processing songs\"):\n",
    "    embedding = get_document_embedding(tokens, fasttext_model.wv, vector_size=100)\n",
    "    doc_embeddings_ft.append(embedding)\n",
    "\n",
    "X_ft = np.array(doc_embeddings_ft, dtype=np.float32)\n",
    "\n",
    "print(f\"\\nFastText document embeddings shape: {X_ft.shape}\")\n",
    "print(f\"Memory usage: {X_ft.nbytes / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c6a3cf",
   "metadata": {},
   "source": [
    "### 7.8 Build FAISS Indexes for Semantic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd4bbca",
   "metadata": {},
   "source": [
    "Create FAISS indexes for both Word2Vec and FastText embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8afb267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FAISS index for Word2Vec embeddings...\n",
      "Word2Vec FAISS index: 57650 vectors, 300 dimensions\n",
      "\n",
      "Building FAISS index for FastText embeddings...\n",
      "FastText FAISS index: 57650 vectors, 100 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Normalize embeddings for cosine similarity\n",
    "X_w2v_norm = normalize(X_w2v, norm='l2')\n",
    "X_ft_norm = normalize(X_ft, norm='l2')\n",
    "\n",
    "# Build FAISS index for Word2Vec embeddings\n",
    "print(\"Building FAISS index for Word2Vec embeddings...\")\n",
    "index_w2v = faiss.IndexFlatIP(X_w2v_norm.shape[1])\n",
    "index_w2v.add(X_w2v_norm)\n",
    "print(f\"Word2Vec FAISS index: {index_w2v.ntotal} vectors, {X_w2v_norm.shape[1]} dimensions\")\n",
    "\n",
    "# Build FAISS index for FastText embeddings\n",
    "print(\"\\nBuilding FAISS index for FastText embeddings...\")\n",
    "index_ft = faiss.IndexFlatIP(X_ft_norm.shape[1])\n",
    "index_ft.add(X_ft_norm)\n",
    "print(f\"FastText FAISS index: {index_ft.ntotal} vectors, {X_ft_norm.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238d7ab3",
   "metadata": {},
   "source": [
    "### 7.9 Semantic Recommendation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7c6c76",
   "metadata": {},
   "source": [
    "Create recommendation functions using Word2Vec and FastText embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "106ded4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_songs_w2v(song_name, df=df, top_k=5):\n",
    "    \"\"\"\n",
    "    Recommend songs using Word2Vec embeddings.\n",
    "    \"\"\"\n",
    "    idx = df[df['song'].str.lower() == song_name.lower()].index\n",
    "    if len(idx) == 0:\n",
    "        return \"Song not found in the dataset.\"\n",
    "    idx = idx[0]\n",
    "    \n",
    "    query = X_w2v_norm[idx].reshape(1, -1)\n",
    "    scores, indices = index_w2v.search(query, top_k + 1)\n",
    "    \n",
    "    result = df[['song', 'artist']].iloc[indices[0][1:]].copy()\n",
    "    result['similarity'] = scores[0][1:]\n",
    "    return result\n",
    "\n",
    "\n",
    "def recommend_songs_fasttext(song_name, df=df, top_k=5):\n",
    "    \"\"\"\n",
    "    Recommend songs using FastText embeddings.\n",
    "    \"\"\"\n",
    "    idx = df[df['song'].str.lower() == song_name.lower()].index\n",
    "    if len(idx) == 0:\n",
    "        return \"Song not found in the dataset.\"\n",
    "    idx = idx[0]\n",
    "    \n",
    "    query = X_ft_norm[idx].reshape(1, -1)\n",
    "    scores, indices = index_ft.search(query, top_k + 1)\n",
    "    \n",
    "    result = df[['song', 'artist']].iloc[indices[0][1:]].copy()\n",
    "    result['similarity'] = scores[0][1:]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4485e6f1",
   "metadata": {},
   "source": [
    "### 7.10 Compare All Methods: TF-IDF vs Word2Vec vs FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e757295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for: 'A Moment Suspended In Time' by A Moment Suspended In Time\n",
      "======================================================================\n",
      "\n",
      "TF-IDF Recommendations:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40633</th>\n",
       "      <td>Dreamin'</td>\n",
       "      <td>Kiss</td>\n",
       "      <td>0.484948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49061</th>\n",
       "      <td>Dreamin' About U</td>\n",
       "      <td>Prince</td>\n",
       "      <td>0.446586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44759</th>\n",
       "      <td>I Must Be Dreaming</td>\n",
       "      <td>Nat King Cole</td>\n",
       "      <td>0.445657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13767</th>\n",
       "      <td>I Must Be Dreaming</td>\n",
       "      <td>Neil Sedaka</td>\n",
       "      <td>0.404857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49275</th>\n",
       "      <td>A Winter's Tale</td>\n",
       "      <td>Queen</td>\n",
       "      <td>0.400612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     song         artist  similarity\n",
       "40633            Dreamin'           Kiss    0.484948\n",
       "49061    Dreamin' About U         Prince    0.446586\n",
       "44759  I Must Be Dreaming  Nat King Cole    0.445657\n",
       "13767  I Must Be Dreaming    Neil Sedaka    0.404857\n",
       "49275     A Winter's Tale          Queen    0.400612"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word2Vec Recommendations:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23900</th>\n",
       "      <td>Right Before Your Eyes</td>\n",
       "      <td>America</td>\n",
       "      <td>0.949501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25783</th>\n",
       "      <td>Can't Wait</td>\n",
       "      <td>Bob Dylan</td>\n",
       "      <td>0.949051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22643</th>\n",
       "      <td>Leaving Me</td>\n",
       "      <td>Zox</td>\n",
       "      <td>0.948582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52601</th>\n",
       "      <td>Not The Moment</td>\n",
       "      <td>Supertramp</td>\n",
       "      <td>0.948522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55012</th>\n",
       "      <td>Finding Me</td>\n",
       "      <td>Vertical Horizon</td>\n",
       "      <td>0.948123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         song            artist  similarity\n",
       "23900  Right Before Your Eyes           America    0.949501\n",
       "25783              Can't Wait         Bob Dylan    0.949051\n",
       "22643              Leaving Me               Zox    0.948582\n",
       "52601          Not The Moment        Supertramp    0.948522\n",
       "55012              Finding Me  Vertical Horizon    0.948123"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FastText Recommendations:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>artist</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34925</th>\n",
       "      <td>Coma</td>\n",
       "      <td>Guns N' Roses</td>\n",
       "      <td>0.975415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14789</th>\n",
       "      <td>Out Of Control</td>\n",
       "      <td>Oingo Boingo</td>\n",
       "      <td>0.974774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22464</th>\n",
       "      <td>Wasted</td>\n",
       "      <td>Zebrahead</td>\n",
       "      <td>0.974675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22436</th>\n",
       "      <td>Morse Code For Suckers</td>\n",
       "      <td>Zebrahead</td>\n",
       "      <td>0.973370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33411</th>\n",
       "      <td>Behind The Lines</td>\n",
       "      <td>Genesis</td>\n",
       "      <td>0.973218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         song         artist  similarity\n",
       "34925                    Coma  Guns N' Roses    0.975415\n",
       "14789          Out Of Control   Oingo Boingo    0.974774\n",
       "22464                  Wasted      Zebrahead    0.974675\n",
       "22436  Morse Code For Suckers      Zebrahead    0.973370\n",
       "33411        Behind The Lines        Genesis    0.973218"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = random.randint(0, len(df))\n",
    "test_song = df['song'].iloc[idx]\n",
    "test_artist = df['song'].iloc[idx]\n",
    "\n",
    "print(f\"Recommendations for: '{test_song}' by {test_artist}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nTF-IDF Recommendations:\")\n",
    "print(\"-\" * 40)\n",
    "display(recommend_songs_tfidf(test_song))\n",
    "\n",
    "print(\"\\nWord2Vec Recommendations:\")\n",
    "print(\"-\" * 40)\n",
    "display(recommend_songs_w2v(test_song))\n",
    "\n",
    "print(\"\\nFastText Recommendations:\")\n",
    "print(\"-\" * 40)\n",
    "display(recommend_songs_fasttext(test_song))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b26823",
   "metadata": {},
   "source": [
    "### 7.11 Save Semantic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85681fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: fasttext_lyrics.model\n",
      "Saved: embeddings_fasttext.npy\n",
      "Saved: embeddings_w2v.npy\n",
      "Saved: faiss_w2v.index\n",
      "Saved: faiss_fasttext.index\n",
      "\n",
      "All semantic models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save FastText model\n",
    "fasttext_model.save(\"../models/fasttext/fasttext_lyrics.model\")\n",
    "print(\"Saved: fasttext_lyrics.model\")\n",
    "\n",
    "# Save FastText embeddings\n",
    "np.save(\"../data/embeddings/embeddings_fasttext.npy\", X_ft)\n",
    "print(\"Saved: embeddings_fasttext.npy\")\n",
    "\n",
    "# Save Word2Vec embeddings (from pre-trained model)\n",
    "np.save(\"../data/embeddings/embeddings_w2v.npy\", X_w2v)\n",
    "print(\"Saved: embeddings_w2v.npy\")\n",
    "\n",
    "# Save FAISS indexes\n",
    "faiss.write_index(index_w2v, \"../models/faiss_indexes/faiss_w2v.index\")\n",
    "print(\"Saved: faiss_w2v.index\")\n",
    "\n",
    "faiss.write_index(index_ft, \"../models/faiss_indexes/faiss_fasttext.index\")\n",
    "print(\"Saved: faiss_fasttext.index\")\n",
    "\n",
    "print(\"\\nAll semantic models saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f544bc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Improvement 2: Sentence-BERT\n",
    "\n",
    "> **Goal**: Use transformer-based embeddings that understand entire sentences/paragraphs, not just individual words.\n",
    "\n",
    "### Why Sentence-BERT?\n",
    "\n",
    "**Word2Vec Limitation vs SBERT Solution:**\n",
    "- Word2Vec averages word vectors (loses word order), SBERT encodes full sentence context\n",
    "- \"I love you\" = \"You love I\" in Word2Vec, but SBERT preserves word order meaning\n",
    "- Word2Vec has fixed vocabulary, SBERT handles any text\n",
    "\n",
    "### Model Options\n",
    "\n",
    "1. **`all-MiniLM-L6-v2`** (recommended)\n",
    "   - 384 dimensions\n",
    "   - Fast inference\n",
    "   - Good quality\n",
    "\n",
    "2. **`all-mpnet-base-v2`** (higher quality)\n",
    "   - 768 dimensions\n",
    "   - Slower but more accurate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10db11b6",
   "metadata": {},
   "source": [
    "### 8.1 Install and Load Sentence-Transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5856574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Install sentence-transformers\n",
    "# !pip install sentence-transformers -q\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e7ffa6",
   "metadata": {},
   "source": [
    "### 8.2 Generate SBERT Embeddings for All Songs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "501952ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Encode all lyrics with SBERT\n",
    "# X_sbert = sbert_model.encode(df['cleaned_text'].tolist(), show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2746efa",
   "metadata": {},
   "source": [
    "### 8.3 Build FAISS Index & Recommendation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d18e568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build FAISS index for SBERT embeddings\n",
    "# TODO: Implement recommend_songs_sbert()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9c5943",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Improvement 3: Audio Features (Spotify API)\n",
    "\n",
    "> **Goal**: Incorporate audio characteristics (tempo, energy, danceability) to capture musical similarity beyond lyrics.\n",
    "\n",
    "### Spotify Audio Features\n",
    "\n",
    "- `danceability`: How suitable for dancing (0.0 - 1.0)\n",
    "- `energy`: Intensity and activity (0.0 - 1.0)\n",
    "- `tempo`: Beats per minute (~50 - 200)\n",
    "- `valence`: Musical positiveness (0.0 - 1.0)\n",
    "- `acousticness`: Acoustic vs electronic (0.0 - 1.0)\n",
    "- `instrumentalness`: Vocal vs instrumental (0.0 - 1.0)\n",
    "\n",
    "### Implementation Steps\n",
    "\n",
    "1. Set up Spotify API credentials\n",
    "2. Fetch audio features for each song\n",
    "3. Normalize and combine with lyrics embeddings\n",
    "4. Create hybrid similarity metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed75922",
   "metadata": {},
   "source": [
    "### 9.1 Set Up Spotify API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d48e1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Install spotipy and set up credentials\n",
    "# !pip install spotipy -q\n",
    "# import spotipy\n",
    "# from spotipy.oauth2 import SpotifyClientCredentials\n",
    "# sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id=\"...\", client_secret=\"...\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139600f4",
   "metadata": {},
   "source": [
    "### 9.2 Fetch Audio Features for Songs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "863e499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Search for tracks and get audio features\n",
    "# def get_audio_features(artist, song):\n",
    "#     results = sp.search(q=f\"artist:{artist} track:{song}\", type='track', limit=1)\n",
    "#     if results['tracks']['items']:\n",
    "#         track_id = results['tracks']['items'][0]['id']\n",
    "#         return sp.audio_features(track_id)[0]\n",
    "#     return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece037b",
   "metadata": {},
   "source": [
    "### 9.3 Create Audio Feature Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "608e2430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Normalize and combine audio features into embeddings\n",
    "# audio_features = ['danceability', 'energy', 'tempo', 'valence', 'acousticness', 'instrumentalness']\n",
    "# X_audio = df[audio_features].values\n",
    "# X_audio = normalize(X_audio, norm='l2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46049175",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Embedding Comparison & Evaluation\n",
    "\n",
    "> **Goal**: Compare TF-IDF, Word2Vec, and SBERT embeddings for lyric-based song similarity. Evaluate which method produces the most meaningful recommendations.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "- **Qualitative**: Manual inspection of recommendation quality\n",
    "- **Diversity**: How diverse are the recommended songs?\n",
    "- **Artist Coverage**: Do recommendations include different artists?\n",
    "- **Semantic Coherence**: Do lyrics share similar themes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002de6fb",
   "metadata": {},
   "source": [
    "### 10.1 Side-by-Side Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03f2dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare recommendations from all methods for the same song\n",
    "# def compare_recommendations(song_name, top_k=5):\n",
    "#     print(f\"Recommendations for: {song_name}\")\n",
    "#     print(\"\\n📊 TF-IDF:\")\n",
    "#     display(recommend_songs_tfidf(song_name, top_k))\n",
    "#     print(\"\\n📊 Word2Vec:\")\n",
    "#     display(recommend_songs_w2v(song_name, top_k))\n",
    "#     print(\"\\n📊 Sentence-BERT:\")\n",
    "#     display(recommend_songs_sbert(song_name, top_k))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a3d295",
   "metadata": {},
   "source": [
    "### 10.2 Embedding Visualization (t-SNE/UMAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1207f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize embeddings in 2D using t-SNE or UMAP\n",
    "# from sklearn.manifold import TSNE\n",
    "# X_embedded = TSNE(n_components=2).fit_transform(X_sbert[:1000])\n",
    "# plt.scatter(X_embedded[:, 0], X_embedded[:, 1], alpha=0.5)\n",
    "# plt.title(\"SBERT Embeddings (t-SNE)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e2fc1",
   "metadata": {},
   "source": [
    "### 10.3 Summary Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bc30c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create summary comparison table\n",
    "# comparison_df = pd.DataFrame({\n",
    "#     'Method': ['TF-IDF', 'Word2Vec', 'FastText', 'SBERT'],\n",
    "#     'Dimensions': [5000, 300, 100, 384],\n",
    "#     'Semantic': ['❌', '✅', '✅', '✅✅'],\n",
    "#     'Speed': ['Fast', 'Fast', 'Fast', 'Slow'],\n",
    "#     'Quality': ['Good', 'Better', 'Better', 'Best']\n",
    "# })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf46bd2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Hybrid Recommendation System\n",
    "\n",
    "> **Goal**: Combine multiple similarity signals (lyrics, genre, artist, popularity) for better recommendations.\n",
    "\n",
    "### Hybrid Score Formula\n",
    "\n",
    "$$\\text{FinalScore} = w_1 \\cdot \\text{LyricSim} + w_2 \\cdot \\text{GenreSim} + w_3 \\cdot \\text{Popularity}$$\n",
    "\n",
    "**Default Weights:**\n",
    "- w1 = 0.5 (Lyrics similarity - most important)\n",
    "- w2 = 0.3 (Genre/Artist similarity)\n",
    "- w3 = 0.2 (Popularity score)\n",
    "\n",
    "### Components to Combine\n",
    "\n",
    "- **Lyrics Similarity**: SBERT/TF-IDF cosine similarity (0.5)\n",
    "- **Artist Similarity**: Same/similar artist bonus (0.1)\n",
    "- **Genre Similarity**: Same genre bonus (0.2)\n",
    "- **Popularity**: Normalized play count (0.1)\n",
    "- **Release Year**: Recency bonus (0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c042b3e1",
   "metadata": {},
   "source": [
    "### 11.1 Lyrics Similarity Component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c61acc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get lyrics similarity scores from SBERT index\n",
    "# def get_lyrics_similarity(song_idx, candidate_indices):\n",
    "#     query = X_sbert[song_idx].reshape(1, -1)\n",
    "#     scores, _ = index_sbert.search(query, len(candidate_indices))\n",
    "#     return scores[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131b4c53",
   "metadata": {},
   "source": [
    "### 11.2 Artist / Genre Similarity Component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa06b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute artist/genre similarity\n",
    "# def get_artist_similarity(song_idx, candidate_indices):\n",
    "#     query_artist = df.iloc[song_idx]['artist']\n",
    "#     similarities = []\n",
    "#     for idx in candidate_indices:\n",
    "#         if df.iloc[idx]['artist'] == query_artist:\n",
    "#             similarities.append(1.0)  # Same artist\n",
    "#         else:\n",
    "#             similarities.append(0.0)\n",
    "#     return np.array(similarities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62342ea0",
   "metadata": {},
   "source": [
    "### 11.3 Popularity / Release Year Component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "576f20b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute popularity scores (if available)\n",
    "# def get_popularity_scores(candidate_indices):\n",
    "#     if 'popularity' in df.columns:\n",
    "#         scores = df.iloc[candidate_indices]['popularity'].values\n",
    "#         return (scores - scores.min()) / (scores.max() - scores.min())\n",
    "#     return np.zeros(len(candidate_indices))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ece7d3",
   "metadata": {},
   "source": [
    "### 11.4 Hybrid Recommendation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a9e504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement hybrid recommendation\n",
    "# def recommend_songs_hybrid(song_name, top_k=5, weights={'lyrics': 0.5, 'genre': 0.3, 'popularity': 0.2}):\n",
    "#     idx = df[df['song'].str.lower() == song_name.lower()].index[0]\n",
    "#     \n",
    "#     # Get initial candidates from lyrics similarity\n",
    "#     _, candidates = index_sbert.search(X_sbert[idx].reshape(1, -1), 100)\n",
    "#     candidates = candidates[0][1:]  # Exclude query song\n",
    "#     \n",
    "#     # Compute component scores\n",
    "#     lyrics_scores = get_lyrics_similarity(idx, candidates)\n",
    "#     artist_scores = get_artist_similarity(idx, candidates)\n",
    "#     popularity_scores = get_popularity_scores(candidates)\n",
    "#     \n",
    "#     # Combine with weights\n",
    "#     final_scores = (weights['lyrics'] * lyrics_scores + \n",
    "#                     weights['genre'] * artist_scores + \n",
    "#                     weights['popularity'] * popularity_scores)\n",
    "#     \n",
    "#     # Return top-k\n",
    "#     top_indices = np.argsort(final_scores)[::-1][:top_k]\n",
    "#     return df.iloc[candidates[top_indices]][['song', 'artist']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413c6b27",
   "metadata": {},
   "source": [
    "### 11.5 Weight Tuning & Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de406557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test different weight combinations\n",
    "# weight_configs = [\n",
    "#     {'lyrics': 0.5, 'genre': 0.3, 'popularity': 0.2},\n",
    "#     {'lyrics': 0.7, 'genre': 0.2, 'popularity': 0.1},\n",
    "#     {'lyrics': 0.4, 'genre': 0.4, 'popularity': 0.2},\n",
    "# ]\n",
    "# \n",
    "# for config in weight_configs:\n",
    "#     print(f\"Weights: {config}\")\n",
    "#     display(recommend_songs_hybrid(test_song, weights=config))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf36be6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Advanced FAISS Indexes\n",
    "\n",
    "> **Goal**: Compare different FAISS index types for speed vs accuracy trade-offs.\n",
    "\n",
    "### Index Types Comparison\n",
    "\n",
    "- **IndexFlatIP**: Exact search (baseline) - Slow, High Memory, 100% Recall\n",
    "- **IndexIVFFlat**: Inverted file index - Fast, Medium Memory, ~95% Recall\n",
    "- **IndexHNSWFlat**: Hierarchical NSW graph - Very Fast, Medium Memory, ~98% Recall\n",
    "- **IndexIVFPQ**: Product quantization - Very Fast, Low Memory, ~90% Recall\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "- **IndexFlatIP**: Small datasets (<100K), need exact results\n",
    "- **IndexIVFFlat**: Medium datasets, good balance\n",
    "- **IndexHNSWFlat**: Large datasets, best quality/speed\n",
    "- **IndexIVFPQ**: Very large datasets, memory constrained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f4a824",
   "metadata": {},
   "source": [
    "### 12.1 Baseline: IndexFlatIP (Current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60fb7564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: Exact search\n",
    "# index_flat = faiss.IndexFlatIP(dim)\n",
    "# index_flat.add(X)\n",
    "# print(f\"IndexFlatIP: {index_flat.ntotal} vectors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61287b6",
   "metadata": {},
   "source": [
    "### 12.2 IndexIVFFlat (Inverted File Index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98a9024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build IndexIVFFlat\n",
    "# nlist = 100  # Number of clusters\n",
    "# quantizer = faiss.IndexFlatIP(dim)\n",
    "# index_ivf = faiss.IndexIVFFlat(quantizer, dim, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "# index_ivf.train(X)\n",
    "# index_ivf.add(X)\n",
    "# index_ivf.nprobe = 10  # Number of clusters to search\n",
    "# print(f\"IndexIVFFlat: {index_ivf.ntotal} vectors, {nlist} clusters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b0d0de",
   "metadata": {},
   "source": [
    "### 12.3 IndexHNSWFlat (Hierarchical NSW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "065ffb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build IndexHNSWFlat\n",
    "# M = 32  # Number of connections per layer\n",
    "# index_hnsw = faiss.IndexHNSWFlat(dim, M, faiss.METRIC_INNER_PRODUCT)\n",
    "# index_hnsw.hnsw.efConstruction = 40  # Construction-time parameter\n",
    "# index_hnsw.hnsw.efSearch = 16  # Search-time parameter\n",
    "# index_hnsw.add(X)\n",
    "# print(f\"IndexHNSWFlat: {index_hnsw.ntotal} vectors, M={M}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6b3ff4",
   "metadata": {},
   "source": [
    "### 12.4 Speed Comparison Benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23626bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Benchmark search speed\n",
    "# import time\n",
    "# \n",
    "# def benchmark_index(index, name, X, n_queries=100, top_k=10):\n",
    "#     queries = X[np.random.choice(len(X), n_queries)]\n",
    "#     start = time.time()\n",
    "#     for q in queries:\n",
    "#         index.search(q.reshape(1, -1), top_k)\n",
    "#     elapsed = time.time() - start\n",
    "#     qps = n_queries / elapsed\n",
    "#     print(f\"{name}: {qps:.1f} queries/sec ({elapsed*1000/n_queries:.2f} ms/query)\")\n",
    "#     return qps\n",
    "# \n",
    "# benchmark_index(index_flat, \"IndexFlatIP\", X)\n",
    "# benchmark_index(index_ivf, \"IndexIVFFlat\", X)\n",
    "# benchmark_index(index_hnsw, \"IndexHNSWFlat\", X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54824aad",
   "metadata": {},
   "source": [
    "### 12.5 Memory Usage Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04f0ec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare memory usage\n",
    "# import sys\n",
    "# \n",
    "# def get_index_size(index):\n",
    "#     faiss.write_index(index, \"/tmp/temp_index\")\n",
    "#     size_mb = os.path.getsize(\"/tmp/temp_index\") / 1024 / 1024\n",
    "#     return size_mb\n",
    "# \n",
    "# print(f\"IndexFlatIP: {get_index_size(index_flat):.2f} MB\")\n",
    "# print(f\"IndexIVFFlat: {get_index_size(index_ivf):.2f} MB\")\n",
    "# print(f\"IndexHNSWFlat: {get_index_size(index_hnsw):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b104c4",
   "metadata": {},
   "source": [
    "### 12.6 Recall@K Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85e35a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute Recall@K (how many of exact top-K are found by ANN)\n",
    "# def compute_recall(index_exact, index_approx, X, n_queries=100, k=10):\n",
    "#     queries = X[np.random.choice(len(X), n_queries)]\n",
    "#     recalls = []\n",
    "#     for q in queries:\n",
    "#         q = q.reshape(1, -1)\n",
    "#         _, exact_ids = index_exact.search(q, k)\n",
    "#         _, approx_ids = index_approx.search(q, k)\n",
    "#         recall = len(set(exact_ids[0]) & set(approx_ids[0])) / k\n",
    "#         recalls.append(recall)\n",
    "#     return np.mean(recalls)\n",
    "# \n",
    "# print(f\"IndexIVFFlat Recall@10: {compute_recall(index_flat, index_ivf, X):.2%}\")\n",
    "# print(f\"IndexHNSWFlat Recall@10: {compute_recall(index_flat, index_hnsw, X):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad7bd8",
   "metadata": {},
   "source": [
    "### 12.7 Performance Comparison Chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c803ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create visualization comparing all indexes\n",
    "# results = pd.DataFrame({\n",
    "#     'Index': ['IndexFlatIP', 'IndexIVFFlat', 'IndexHNSWFlat'],\n",
    "#     'Speed (QPS)': [100, 1000, 5000],\n",
    "#     'Memory (MB)': [500, 550, 600],\n",
    "#     'Recall@10': [1.0, 0.95, 0.98]\n",
    "# })\n",
    "# \n",
    "# fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "# results.plot.bar(x='Index', y='Speed (QPS)', ax=axes[0], color='steelblue')\n",
    "# results.plot.bar(x='Index', y='Memory (MB)', ax=axes[1], color='coral')\n",
    "# results.plot.bar(x='Index', y='Recall@10', ax=axes[2], color='seagreen')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6ad12a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Upload Models to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b720c78",
   "metadata": {},
   "source": [
    "Upload all embeddings, FAISS indexes, and models to Hugging Face for easy deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e499b3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository 'namviet157/music-recommendation' ready!\n"
     ]
    }
   ],
   "source": [
    "login()  # This will prompt for your token\n",
    "\n",
    "# Initialize API\n",
    "api = HfApi()\n",
    "\n",
    "# Your Hugging Face username/repo\n",
    "REPO_ID = \"namviet157/music-recommendation\"  # Change this to your repo\n",
    "\n",
    "# Create repository if it doesn't exist\n",
    "try:\n",
    "    api.create_repo(repo_id=REPO_ID, repo_type=\"model\", exist_ok=True)\n",
    "    print(f\"Repository '{REPO_ID}' ready!\")\n",
    "except Exception as e:\n",
    "    print(f\"Repo exists or error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8d42ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading files to Hugging Face Hub...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\min_ds-env\\Lib\\site-packages\\huggingface_hub\\hf_api.py:4180: UserWarning: It seems that you are about to commit a data file (df_cleaned.parquet) to a model repository. You are sure this is intended? If you are trying to upload a dataset, please set `repo_type='dataset'` or `--repo-type=dataset` in a CLI.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367a4c128f3343aab0ac38a2dbec96c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8213694eef6c414ba9fca515c5e3ffb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: df_cleaned.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94df66270f842d7961d39930423f4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5673ec621f44c190be35606c4f73f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: embeddings_tfidf.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e083b22d004288ae415a74c40a02fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d120122d558d41d58b215d153537cd01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: embeddings_fasttext.npy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150cd4d0dc3a41bf9273304d7fa91d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb8604b38704fc08e4281aa4b30b707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: embeddings_w2v.npy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01538e11df9646278f318e2532d739c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db0dbfd25d74eada2987aa419f44b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: faiss_tfidf.index\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55987241ad840408c59f5339682c2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db18cd377f6f4b10b32eda5701a1d2d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: faiss_fasttext.index\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d8dc1833a64ab28bf76cf9bbed8053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6b4e08b43a4d43b4753fd0873a2e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded: faiss_w2v.index\n",
      "Uploaded: requirements.txt\n",
      "\n",
      "All files uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Upload all files to Hugging Face Hub\n",
    "\n",
    "files_to_upload = [\n",
    "    # DataFrame\n",
    "    (\"../data/processed/df_cleaned.parquet\", \"df_cleaned.parquet\"),\n",
    "    \n",
    "    # Embeddings\n",
    "    (\"../data/embeddings/embeddings_tfidf.npz\", \"embeddings_tfidf.npz\"),\n",
    "    (\"../data/embeddings/embeddings_fasttext.npy\", \"embeddings_fasttext.npy\"),\n",
    "    (\"../data/embeddings/embeddings_w2v.npy\", \"embeddings_w2v.npy\"),\n",
    "    \n",
    "    # FAISS indexes\n",
    "    (\"../models/faiss_indexes/faiss_tfidf.index\", \"faiss_tfidf.index\"),\n",
    "    (\"../models/faiss_indexes/faiss_fasttext.index\", \"faiss_fasttext.index\"),\n",
    "    (\"../models/faiss_indexes/faiss_w2v.index\", \"faiss_w2v.index\"),\n",
    "\n",
    "    # requirements.txt\n",
    "    (\"../requirements.txt\", \"requirements.txt\"),\n",
    "]\n",
    "\n",
    "print(\"Uploading files to Hugging Face Hub...\")\n",
    "for local_path, hf_path in files_to_upload:\n",
    "    try:\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=local_path,\n",
    "            path_in_repo=hf_path,\n",
    "            repo_id=REPO_ID,\n",
    "            repo_type=\"model\"\n",
    "        )\n",
    "        print(f\"Uploaded: {hf_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload {hf_path}: {e}\")\n",
    "\n",
    "print(\"\\nAll files uploaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1671cf98",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Final Summary\n",
    "\n",
    "### Implementation Progress\n",
    "\n",
    "**Core (Complete):**\n",
    "- TF-IDF Baseline\n",
    "- FAISS IndexFlatIP\n",
    "- Basic Recommendation\n",
    "\n",
    "**Embedding (TODO):**\n",
    "- Word2Vec/FastText\n",
    "- Sentence-BERT\n",
    "- Audio Features\n",
    "- Comparison & Eval\n",
    "\n",
    "**Hybrid (TODO):**\n",
    "- Multi-signal Fusion\n",
    "\n",
    "**FAISS (TODO):**\n",
    "- IndexIVFFlat\n",
    "- IndexHNSWFlat\n",
    "- Benchmarks\n",
    "\n",
    "### Key Files\n",
    "\n",
    "```\n",
    "models/\n",
    "├── faiss_indexes/\n",
    "│   ├── faiss.index          # TF-IDF baseline\n",
    "│   ├── faiss_sbert.index    # Sentence-BERT (TODO)\n",
    "│   └── faiss_hnsw.index     # HNSW optimized (TODO)\n",
    "├── fasttext_lyrics.model    # Custom FastText (TODO)\n",
    "└── sbert_embeddings.npy     # SBERT embeddings (TODO)\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Implement Word2Vec/FastText embeddings\n",
    "2. Add Sentence-BERT for best quality\n",
    "3. Build hybrid recommendation system\n",
    "4. Optimize with advanced FAISS indexes\n",
    "5. Deploy as web application\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
